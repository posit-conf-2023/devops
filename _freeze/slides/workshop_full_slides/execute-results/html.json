{
  "hash": "e2c56e1a1dd5b96ef1ef8eb4e11f1789",
  "result": {
    "markdown": "---\ntitle: \"Devops for Data Scientists\"\nsubtitle: \"wifi name:xyz wifi password:123 \"\ntitle-slide-attributes: \n  data-background-color: white\n  data-background-image: _extensions/positconfslides/assets/backgrounds/toc-light.svg\n  data-background-size: contain\nformat:\n  positconfslides-revealjs: \n    chalkboard: true\n    slide-number: true\n    footer: <https://github.com/posit-conf-2023/devops>\n    incremental: false\n    code-copy: true\n    center-title-slide: false\n    code-link: true\n    code-overflow: wrap\n    highlight-style: a11y\n    width: \"1600\"\n    height: \"900\"\n    filters:\n      - positconfslides\n---\n\n## Part 1: Introductions, setup, & workshop overview\n\n## Workshop Goals\n\n-   To understand how devops can help you in your work as data scientists\n\n-   To understand the main principles of Devops\n\n-   To get hands-on experience putting code into production using common devops workflows\n\n-   To leave the workshop with some \"assets\" and skills you can use in your work\n\n## Agenda & Lab Overview\n\n+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Section/Time                       | Topics                                               | Labs                                                                                                                                                          |\n+====================================+======================================================+===============================================================================================================================================================+\n| Part 1                             | Workshop overview                                    | Infrastructure & wifi setup                                                                                                                                   |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Logistics & setup                                    | Optional: Linux Refresher                                                                                                                                     |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Introductions                                        |                                                                                                                                                               |\n+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Part 2: Devops Principles          | Introduction to devops                               | Lab 1: Deploy your own Quarto website on Github Pages using GitHub Actions CI/CD                                                                              |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Version control & github                             |                                                                                                                                                               |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | CI/CD                                                |                                                                                                                                                               |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Reproducing workflows and environments               |                                                                                                                                                               |\n+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Part 3: Docker for Data Scientists | How and why data scientists use docker in production | Lab #2: Write your own Dockerfile to deploy Open Source Shiny Server on [Docker playground](https://labs.play-with-docker.com/) and host an app on the server |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Docker: overview and architecture                    |                                                                                                                                                               |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Building docker images and containers                |                                                                                                                                                               |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Ports & networking                                   |                                                                                                                                                               |\n+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Part 4: Data Science in Production | Choosing your deployment method                      | Lab #3: Create an API wrapper around a trained prediction model and host it on Posit Connect                                                                  |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | APIs and when to use them                            |                                                                                                                                                               |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Just enough auth                                     |                                                                                                                                                               |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Logging & metrics & testing                          |                                                                                                                                                               |\n+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Part 5: Discussion                 | Course feedback                                      | None                                                                                                                                                          |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Questions for the team                               |                                                                                                                                                               |\n|                                    |                                                      |                                                                                                                                                               |\n|                                    | Working with other teams                             |                                                                                                                                                               |\n+------------------------------------+------------------------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n## Supplemental Materials\n\n-   Linux\n-   vim\n-   Git\n-   Cheat sheets\n\n## Most important workshop commands\n\n-   cd\n-   ls\n-   pwd\n-   touch\n-   mkdir\n-   vim\n-   curl\n-   echo\n-   env\n-   \\$PATH\n\n## What we won't cover\n\n-   How to become a devops engineer\n\n-   Python-based workflows\n\n-   In-depth security and auth practices\n\n## Meet your instructors\n\n::: panel-tabset\n## Rika\n\n![](images/rika_bio-01.jpeg){width=\"25%\"}\n\n-   Solutions Engineer at Posit\n\n-   Former Data Scientist and Data Engineer\n\n## Andrie\n\n## David\n\n![](images/edavidaja.jpeg){width=\"25%\"}\n\n-   David Aja is a Solutions Engineer at Posit. Before joining Posit, he worked as a data scientist in the public sector.\n\n## Gagan\n\n![](images/gagan.png){width=\"25%\"}\n\n-   Gagandeep Singh is a former software engineer and data scientist who has worked in a variety of cross-technology teams before joining Posit as a Solutions Engineer.\n\n## Sam\n\n![](images/sam-workweek-2023-large.jpeg){width=\"25%\"}\n\n-   Sam Edwardes is a Solutions Engineer at Posit. He is passionate about open source software and data science. Before joining Posit Sam consulted for many different companies as a Consultant at Deloitte.\n:::\n\n## Solutions Engineering at Posit\n\n![](images/sol-eng.png){width=\"25%\"}\n\n-   Posit's Solutions Engineering team aims to shrink the distance between the needs of Posit's customers and our Pro and Open Source offerings, leading with curiosity and technical excellence.\n\n-   Our customer-facing work helps our customers deploy, install, configure, and use our Pro products.\n\n## Special Thanks to Alex Gold!\n\n![](images/_40A8864.jpg){width=\"206\" height=\"140\"}\n\nAuthor of [Devops for Data Science](www.do4ds.com)\n\nPosit Solutions Engineering Team Director\n\n## Introduce yourself to your neighbor\n\nTake 5 minutes and introduce yourself to your neighbors.\n\n-   How do you think devops can help you in your work?\n\n-   What are you most looking forward to in the conference?\n\n## Logistics & Workshop Setup {.toc-people-dark}\n\n## Pre-workshop Install {.brackets-light}\n\nWe encourage you to set up the following systems prior to the start of the workshop. We will also set aside time during the workshop to install and troubleshoot.\n\n1.  Install [git](https://git-scm.com/downloads) on your laptop. You can check if git is already installed by typing `git --version` in the terminal.\n2.  If you do not have a github account please create a [personal](https://docs.github.com/en/get-started/signing-up-for-github/signing-up-for-a-new-github-account) account.\n3.  Make sure that you have a text editor that you are comfortable using. For example, Rstudio [IDE](https://www.rstudio.com/categories/rstudio-ide/), [vscode](https://code.visualstudio.com/download), [sublime](https://www.sublimetext.com/3).\n4.  Create an account at [Docker Hub](https://hub.docker.com/).\n5.  Download [Docker Desktop](https://www.docker.com/products/docker-desktop/).\n6.  Download [quarto](https://quarto.org/docs/download/).\n7.  Sign up for our workshop Discord channel\n\n## Wifi & Workshop Install {.brackets-light}\n\nWifi Name:\n\nWifi: Password:\n\nIs your wifi working? If not, let one of the instructors/TA's know as soon as possible.\n\n## Workshop Install\n\nLogin to http://rstd.io/class, use the provided username and password and access Posit Workbench and Posit Connect.\n\nLogin to Docker Playground with your Docker Hub username and password.\n\n## Documentation & Communication {.brackets-light}\n\n-   All documents including slides are available at this github [repo](https://github.com/posit-conf-2023/devops)\n-   We will use the Discord channel for communicating code snippets and answering questions\n-   \"Posit\" Notes system for TA support during exercises and labs\n    -   üü• - I need help\n    -   üü® - I'm still working\n    -   üü© - I'm done\n\n## Daily Schedule {.brackets-light}\n\nüìÖ September 17 and 18, 2023\\\n‚è∞ 09:00 - 17:00\\\nüè® ROOM: TBD\\\n‚úçÔ∏èTBD\n\n+---------------+------------------+\n| Time          | Topic            |\n+===============+==================+\n| 10:30 - 11:00 | Break            |\n+---------------+------------------+\n| 12:30 - 13:30 | Lunch            |\n+---------------+------------------+\n| 15:00 - 15:30 | Break            |\n+---------------+------------------+\n\n## Code of Conduct {.brackets-light}\n\nhttps://posit.co/code-of-conduct/\n\n## Part 2: Devops Principles\n\n## Principles of Devops\n\n![](images/Screenshot%202023-08-29%20at%203.32.37%20PM.png){width=\"586\"}\n\n-   Collaboration\n\n-   Automation\n\n-   CI/CD\n\n-   Rapid feedback loops\n\n-   Culture change\n\n::: notes\n**DevOps** is a combination of software development (dev) and operations (ops). It is defined as a software engineering methodology which aims to integrate the work of development teams and operations teams by facilitating a culture of collaboration and shared responsibility.\n\n-   Development of system of practice in the late 2000's in software and IT online communities\n\n-   Developers often siloed from operations teams resulting in inefficient processes\n\n-   Long development cycles and monolithic environments made it difficult to quickly iterate and deploy\n\n-   Software stacks used different languages, frameworks, databases\n\n-   Devops built on agile methodologies used in software dev and extended it to operations\n\n-   Microservices architectures become more popular\n\n-   Movement to integrate security into devops cycles\n:::\n\n::: footer\nIllustration & Definition credit:Gitlab, https://about.gitlab.com/topics/devops/\n:::\n\n## Has this ever happened to you?\n\n::: incremental\n-   You come back to code from a year ago and now it doesnt run!\n\n-   Your boss asks you to share that Shiny app with a client but the dev team is too busy working on their roadmap to help you deploy it somewhere.\n\n-   You need to share your prediction model (created in R) with the Engineering team but they only code in Java.\n\n-   What other problems have you ran into that could possibly be solved with devops practices or tools?\n:::\n\n## Why should data scientists/analysts care about devops?\n\n:::incremental\n-   Building and delivering software includes models, visualizations, data transformations & data applications\n\n-   Practices such as automation, collaboration, testing, and iterative improvement can dramatically improve data work and improve reproducibility\n\n-   Developers of code are often also asked to help \"operationalize\" their code\n\n-   Improve collaboration & communication with engineering, ops, IT, and security teams\n:::\n\n## Responsibility of the analyst\n\n![](images/Screenshot%202023-08-22%20at%203.26.22%20PM.png)\n\n:::notes\n2005 essay on crisis in scientific publishing, argument that majority medical research studies cannot be replicated. \n\nSolo, siloed investigator limited to small sample sizes\nNo preregistration of hypotheses being tested\nPost-hoc cherry picking of hypotheses with best P values\nOnly requiring P < .05\nNo replication\nNo data sharing\n:::\n\n## The CI/CD Pipeline\n\n![](images/cicd.png)\n\n::: notes\n-Automation is a core principle for devops\n-CI/CD is a critical component to making that happen.\n-CI/CD comprises of continuous integration and continuous delivery or continuous deployment. \n-Put together, they form a \"CI/CD pipeline\"---a series of automated workflows that help DevOps teams cut down on manual tasks:\n\n-Continuous integration (CI) automatically builds, tests, and integrates code changes within a shared repository; \n-Continuous delivery (CD) automatically delivers code changes to production-ready environments for approval; \n-Continuous deployment (CD) automatically deploys code changes to customers directly.\n:::\n\n## CI/CD Toolkit\n\n-   Environments\n-   Version Control & Git workflows\n-   Build tools\n    -   YAML\n    -   package managers\n    -   CI/CD platforms\n    \n:::notes\nbuild tools for automating workflows, creating configurations for those workflows, and tools to reproduce environments consistently across workflows\n:::\n\n## Environments\n\n:::notes\nContent is deployed (and code is promoted) across different environments with different intended audiences.\n:::\n\n+-----------------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------+\n| Dev                                                                   | Test                                                                     | Prod                           |\n+=======================================================================+==========================================================================+================================+\n| a place for data scientists to do exploratory analysis and experiment | as similar to prod as possible                                           | separate from dev and test     |\n|                                                                       |                                                                          |                                |\n| often just your local desktop                                         | code testing                                                             | created using code             |\n|                                                                       |                                                                          |                                |\n| data science \"sandbox\" with data that's as close to real as possible  | data validation                                                          | code promotion process + tests |\n|                                                                       |                                                                          |                                |\n| access to R/Python packages                                           | in software dev world includes integration, unit, and regression testing | completely automatic           |\n+-----------------------------------------------------------------------+--------------------------------------------------------------------------+--------------------------------+\n\n## Version Control & Workflows\n\n![](images/Screenshot%202023-08-23%20at%2011.20.04%20AM.png)\n\n::: notes\nVersion control is a main tool for Continuous Integration - \n\nlots of variants - giit, github., svn, gitlab, etc\nan an iterative process to build, test, collaborate on your code to above environments. Very commonly, individuals work on separate branches that are then tested and reviewed by colleagues before they are merged into a main branch.\n\nIn addition to the action of promoting your code - its also important to have processes in place for how it happens\ncode reviews, process for your team, how to name things, pull requests, version control with git, feature branching, automatic tests\n:::\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nPart 1 of the Deploy Quarto Site with GHA Lab\n\n## Build tools\n\n:::notes\ntools that automate the process of building and deploying your code once it goes from your dev env to test and prod \n\nTools include config files, `config` package, CI/CD software such as GHA, automatic tests\n:::\n\n![](images/Screenshot%202023-08-23%20at%202.49.21%20PM.png)\n\n::: footer\nIllustration credit:\n:::\n\n## Power of YAML\n\n-   YAML Ain't Markup Language\n\n-   communication of data between people and computers.\n\n-   human friendly\n\n-   everything is a key value pair and interpreted as maps or dictionaries\n\n-   used for configuration files across many execution environments including Docker, virtual machines, K8s, helm, IaaS, etc\n\n## XML {.smaller}\n\n\n```{xml}\n<?xml version=\"1.0\" encoding=\"UTF-8\" ?>\n<root>\n  <row>\n    <name>William</name>\n    <dob>1962</dob>\n    <nickname>Axl Rose</nickname>\n    <instruments>vocals</instruments>\n    <instruments>piano</instruments>\n    <last_name>Bailey</last_name>\n  </row>\n  <row>\n    <name>Saul</name>\n    <dob>1965</dob>\n    <nickname>Slash</nickname>\n    <instruments>guitar</instruments>\n    <last_name>Hudson</last_name>\n  </row>\n</root>\n```\n\n\n## JSON\n\n\n```{json}\n[\n  {\n    \"name\": \"William\",\n    \"last name\": \"Bailey\",\n    \"dob\": 1962,\n    \"nickname\": \"Axl Rose\",\n    \"instruments\": [\n      \"vocals\",\n      \"piano\"\n    ]\n  },\n  {\n    \"name\": \"Saul\",\n    \"last name\": \"Hudson\",\n    \"dob\": 1965,\n    \"nickname\": \"Slash\",\n    \"instruments\": [\n      \"guitar\"\n    ]\n  }\n]\n```\n\n\n\n## YAML\n\n\n```{yaml}\n- name: William\n  last name: Bailey\n  dob: 1962\n  nickname: Axl Rose\n  instruments:\n    - vocals\n    - piano\n\n- name: Saul\n  last name: Hudson\n  dob: 1965\n  nickname: Slash\n  instruments:\n    - guitar\n```\n\n\n## YAML syntax\n\n-   whitespace indentation is used to denote structure, no need for quotes nor brackets\n\n-   Colons separate keys and their values\n\n-   Dashes are used to denote a list\n\n-   https://github.com/sd031/yaml-crash-course/blob/main/full_example.yaml\n\n## Examples\n\n-   https://github.com/rstudio/rstudio-docker-products/blob/dev/docker-compose.yml\n-   https://github.com/Rikagx/personal-website/blob/main/.github/workflows/publish.yaml\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nInspect your \\_quarto.yml file and identify what each part of it does using the quarto site.\n\n## Layers of reproducibility\n\n+---------------+---------------------------------+\n| Layer         | Contents                        |\n+===============+=================================+\n| Packages      | R + Python Packages             |\n+---------------+---------------------------------+\n| System        | R + Python Language Versions    |\n|               |                                 |\n|               | System Libraries                |\n|               |                                 |\n|               | Operating System + dependencies |\n+---------------+---------------------------------+\n| Hardware      | Virtual Hardware                |\n|               |                                 |\n|               | Physical Hardware               |\n+---------------+---------------------------------+\n\n::: notes\n-   Let's say someone wanted to reproduce your project including your code and your environment. Make a list of the layers that would need to be reproduced on their machine. (For example, a layer would be the version of R that you're using)\n\noptions(\"repos\") Run .libPaths()\n\n-   Hints:\n\n    -   Inspect your renv.lock file.\n    -   Where are your packages being pulled from? Confirm by typing `options(\"repos\")`.\n        -   You can modify your package repository by running `options(\"repos\" = c(\"<REPO-NAME>\" = \"https://your-repository-url.com\"))` in your RStudio console.\n    -   Visit the webpage where packages are being pulled from and see if you can identify package dependencies. Are packages downloaded as binaries or from source?\n    -   What are your server and OS dependencies? (If you are not sure which distribution of Linux you are using, you can find it by typing `cat /etc/*-release` in your terminal)\n\n-   In your day-to-day work, what's the hardest reproducibility challenge?\n:::\n\n## Packages vs. Libraries vs. Repositories\n\n**Package** - contains code, functions, data, and documentation. Can be be distributed as SOURCE (a directory with all package components), [BINARIES](https://solutions.posit.co/envs-pkgs/environments/repositories/index.html#binary-packages) (contains files in OS-specific format) or as a BUNDLE (compressed file containing package components, similar to source).\n\n**Library** - is a directory where packages are installed. You can have user-level or project-level libraries. Run `.libPaths()` to see yours. To use a package in has to be installed in a library with `install.packages()` and then loaded into memory with `library(x)` .\n\n**Repository** - a collection of packages. CRAN is a public external repository that is a network of servers that distribute R along with R packages.\n\n## Renv workflow\n\n![](images/Screenshot%202023-09-06%20at%208.31.22%20PM.png)\n\n::: notes\n1.  Use a version control system e.g.[git](https://git-scm.com/) with [GitHub](https://github.com/)\n\n2.  One user (perhaps yourself) should explicitly initialize `renv` in the project, via [`renv::init()`](https://rstudio.github.io/renv/reference/init.html). This will create the initial `renv` lockfile, and also write the `renv` auto-loaders to the project's `.Rprofile` and `renv/activate.R`. These will ensure the right version of `renv` is downloaded and installed for your collaborators when they start in this project.\n\n3.  Using a branching strategy push your code alongside the generated lockfile `renv.lock`. Be sure to also share the generated auto-loaders in `.Rprofile` and `renv/activate.R`.\n\n4.  When a collaborator first launches in this project, `renv` should automatically bootstrap itself, thereby downloading and installing the appropriate version of `renv` into the project library. After this has completed, they can then use [`renv::restore()`](https://rstudio.github.io/renv/reference/restore.html) to restore the project library locally on their machine.\n:::\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nComplete Part 2 of Lab: Deploy Quarto with GHA including the exercises\n\n## Mechanisms for reproducibility\n\n\\*\\* make this a diagram starting from least to most reproducibility\n\n-   documenting state & version control\n\n-   virtual environments (`renv`, `venv`) + package management\n\n-   Containerization & docker\n\n-   Infrastructure as Code\n\n## Is version control secure?\n\n-   our code is still saved locally\n-   How do we make sure that the code we push to Github (or elsewhere) is secure?\n\n## A short auth teaser\n\n-   We can use a variety of data sharing \"transfer protocols\"\n-   protocols specify what kind of traffic is moving between 2 machines\n-   a port specifies where to direct the traffic\n\n| http    | https    | SSH     | git       |\n|---------|----------|---------|-----------|\n| port 80 | port 443 | port 22 | port 9418 |\n\n## Image of ports and how they work\n\n-   notes: A port is a virtual point where network connections start and end. Ports are software-based and managed by a computer's operating system. Each port is associated with a specific process or service. Ports allow computers to easily differentiate between different kinds of traffic: emails go to a different port than webpages, for instance, even though both reach a computer over the same Internet connection.\n\n-   Add picture of client box, server box connected by line with different ports for each box\n\n-   Localhost: `ping localhost - this computer/host`\n\n![](images/ports-01.jpg)\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nPart 3 of Lab: Deploy Quarto with GHA including the exercises\n\n## Automating your build\n\n-   GHA is one tool to automate developer workflows\n-   CI/CD is just one example of these workflows\n-   Run on github servers\n\n::: notes\n-   A github action allows us to create workflows that are triggered by a github action such as a push or pull to a branch\n\n-   Workflows can include tests, markdown renders, shell scripts, cron jobs, or deployments. They can be as simple or as complicated as you need. Open-source community provides a ton of examples of actions.\n\n-   Open source collection of \"available\" actions\n\ncreate a blank repo and show actions workflows https://github.com/Rikagx/workshop_testing/blob/master/.github/workflows/publish.yaml github.com/actions https://github.com/r-lib/actions\n:::\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nPart 4 of Lab: Deploy Quarto with GHA including the exercises\n\n## Part 3: Docker for Data Scientists\n\n## Introduction to Docker\n\n-   Open-source tool.\n-   Package applications and its dependencies in a unit called a container.\n-   Create isolated environments for different experiments.\n-   Share work with colleagues without environment setup issues.\n\n::: notes\nDocker is a tool that allows you to virtualize (put your computer in the cloud) everything you need to create an application or in this case a data science product You can share containers with colleagues without requiring them to have to set up their own local machines Without something like this, in order to recreate or test code that someone else wrote, you'd need every developer to download the same dependencies, configurations, scripts and make sure that it ran on their machine - whether thats a mac, or windows, or linux or some other operating system\n:::\n\n## How can docker help data scientists?\n\n## Portability\n\n-   diagram r code creates model/api in docker container\n-   uses gha to create docker image\n\n## Isolation\n\n## Consistency\n\n::: notes\n-   Data scientists benefit from Docker's consistency and reproducibility.\n\n-   Create isolated environments for different experiments.\n\n-   Share work with colleagues without environment setup issues.\n\n-   **Consistency:** Containers ensure that applications run the same way across different environments.\n\n-   **Isolation:** Containers isolate applications and their dependencies, preventing conflicts.\n\n-   **Portability:** Containers can run on any system that supports Docker, reducing \"it works on my machine\" issues.\n:::\n\n## Lifecycle\n\n![](images/lifecycle.png)\n\n::: footer\nIllustration credit: Alex Gold, do4ds.com\n:::\n\n## Lifecycle Example\n\n``` bash\ndocker pull postgres:12\ndocker pull postgres:latest\ndocker container ls -a\ndocker run -d -e POSTGRES_PASSWORD=mysecretpassword --name postgres_early imageID\ndocker run -d -e POSTGRES_PASSWORD=mysecretpassword --name postgres_new imageID\ndocker container ls -a\ndocker stop\ndocker restart\n```\n\n::: footer\nüîç Live code\n:::\n\n::: notes\na dockerfile is the recipe to build a docker images this recipe is stored in some sort of repository, this can be private, public, or dockerhub - which is the default a docker image contains lightweight instructions to create your application. Docker images use something called layers which makes it super easy and quick to update. Layers start at a base layer which is usually the linux operating system and then they go up to the application layer. a container is the environment for a running process of an image - so if an image is running then its using a container instance. You can have multiple containers running at the same time. But once you delete it everything inside of it goes away.\n\nLet's see an example of what this looks like in practice. Lets say we need to use a postgres sql database for some testing - but we want to test using an older and a newer version of postgres.\n\nLets go to Dockerhub and search for it. We can see official images but there are also thousands of them created by people- hub only has images not dockerfiles or containers themselves. docker image ls - lets list all the images that we have docker pull postgres:12 - see how its pulling and extracting all these layers - but what if we want a newer version or what if we need to run both versions on our machine\n\ndocker pull postgres:latest - notice how some of these layers already exist so it takes a lot less time\n\nLets go on dockerhub and see if we can understand a bit more about the layers https://hub.docker.com/layers/library/postgres/12/images/sha256-a97fd76ab09599e2ddc15c90a87f9a4a2a60551d99f8e7397f12a1d606d7f0ab?context=explore -\n\nwe can see that there are a lot of layers but its hard to understand what exactly is happening - lets look at the dockerfile that shows us the recipe for postgres - this is usually saved in a [github](https://github.com/docker-library/docs/blob/master/postgres/README.md) repo - not on dockerhub which is just images\n\nevery docker file starts with a from command - this is the base layers that starts the image, then we are running some things, copying , env variables, and starting\n\nlets run the image and see\n:::\n\n## Virtual Machine vs. Container\n\n![](images/Screenshot%202023-09-03%20at%209.56.54%20AM.png)\n\n::: notes\ncontainers are very lightweight which makes it really easy and quick to spin them up\n\nthis is because the container itself doesnt have a host operating system or any hardware - intel chip, apple chip etc - this is in the docker engine or runtime which we will look at in the architecture\n:::\n\n## Docker Architecture\n\n![](images/Screenshot%202023-08-30%20at%2011.31.56%20AM.png)\n\n::: notes\n-   Docker uses a client-server architecture. The Docker client talks to the Docker daemon.\n\n-   The Docker client - is the primary way that users interface with Docker via CLI\n\n-   The Docker daemon does the heavy lifting of building, running, and distributing your Docker containers\n\n-   The Docker client and daemon can run on the same system, or you can connect a Docker client to a remote Docker daemon. The Docker client and daemon communicate using a REST API, over UNIX sockets or a network interface.\n\n-   Registry is a Repository for Docker images (e.g., Docker Hub) where you can store and share images.\n\n-   Docker engine is a container runtime that runs on diffrent OS's. Set up the isolated environment for your container\n:::\n\n## Mode for running containers\n\n+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Mode                                                       | Run command       | Use case                                                                                                                                                                                                           |\n+============================================================+===================+====================================================================================================================================================================================================================+\n| Detached                                                   | `docker run -d`   | This runs the container in the **background** so the container keeps running until the application process exits, or you stop the container. Detached mode is often used for production purposes.                  |\n+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Interactive + terminal                                     | `docker run -it`  | This runs the container in the **foreground** so you are unable to access the command prompt. Interactive mode is often used for development and testing.                                                          |\n+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n| Remove everything once the container is done with its task | `docker run --rm` | This mode is used on foreground containers that perform **short-term tasks** such as tests or database backups. Once it is removed anything you may have downloaded or created in the container is also destroyed. |\n+------------------------------------------------------------+-------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nComplete Lab 2: Part 1\n\n## Container Debugging\n\n-   Interactive mode\n\n-   `docker exec`\n\n-   Logging\n\n```         \ndocker run -it -d ubuntu\ndocker container ls -a \ndocker exec -it CONTAINER_ID bash\n\ndocker container run -d --name mydb \\\n --name mydb \\\n -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ \n mysql\n \n docker container logs mydb\n```\n\n::: footer\nüîç Live code\n:::\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nComplete Lab 2: Part 2\n\n## Port Mapping with `docker run -p`\n\n```         \ndocker pull httpd:alpine\ndocker pull httpd:latest\n\ndocker inspect --format='{{.Config.ExposedPorts}}' httpd:latest\ndocker inspect --format='{{.Config.ExposedPorts}}' httpd:alpine\n\ndocker run -p DockerHostPort:ApplicationPort\n\ndocker run -d -p 81:80 --name httpd-latest httpd:latest\ncurl http://localhost:81\n\ndocker run -d -p 80:80 --name httpd-alpine httpd:alpine\ncurl http://localhost:80\n```\n\n::: footer\nüîç Live code\n:::\n\n## Persisting data with Docker\n\nStateless vs. Stateful applications volume mount bind mount external storage shared file system\n\n## Putting it all together\n\nhttps://github.com/rstudio/rstudio-docker-products/blob/dev/docker-compose.yml\n\n::: notes\n-   **docker-compose:** A tool for defining and running multi-container Docker applications.\n-   Uses a YAML file to define services, networks, and volumes.\n-   Simplifies the orchestration of complex applications.\n:::\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nComplete Lab 2: Part 3 Complete Lab 2: Part 4\n\n## Building Docker Images\n\n-   Images are build using a Dockerfile or interactively \"on-the-fly\"\n\n-   Steps to version and share your images on Dockerhub (or a different repository)\n\nmake this a diagram Step 1: Commit Step 2: Tag Step 3: Push\n\n## Lab Activity\n\nüü• - I need help\n\nüü® - I'm still working\n\nüü© - I'm done\n\nComplete Lab 2: Part 5\n\n## Dockerfile Build Commands\n\n| Command | Description |\n|---------|-------------|\n| FROM    |             |\n| ENV     |             |\n| COPY    |             |\n| RUN     |             |\n|         |             |\n|         |             |\n|         |             |\n\n## Dockerfile Example\n\nWalk through Posit Connect [Dockerfile](https://github.com/rstudio/rstudio-docker-products/blob/dev/connect/Dockerfile.ubuntu2204)\n\n## Activity\n\nComplete Lab 2: Part 6\n\n## Part 4: Data Science in Production\n\n## Data Science in Production {.content-dark}\n\n-   Presentation Layer\n\n-   Processing Layer\n\n-   Data store Layer\n\n## Choosing the right presentation layer\n\n-   [Alex's flow chart](https://do4ds.com/chapters/sec1/1-2-proj-arch.html)\n\n-   credit text\n\n## Production \"State\"\n\nQuestions to ask once your content is able to be consumed by your intended audience\n\n-   Is it reproducible?\n\n-   Is it portable?\n\n-   Is it maintainable?\n\n-   Does it scale?\n\n-   Is your code efficiently written?\n\n-   Is it secure and accessible?\n\n-   Can you trust your code?\n\n## APIs can help\n\n-   RESTful APIs\n\n-   a way to access content by non-R users\n\n## Structure of an API\n\n## Activity: Create a Posit Connect API key\n\n-   login to Connect\n\n-   create connect key\n\n-   use rsconnect to access\n\n-   use curl to access\n\n## Creating an API\n\n-   plumber exercise\n\n## Securing your application\n\n-   SSL & https\n\n-   live code\n\n## Authentication vs. Authorization\n\n## Deploying and hosting your API\n\n## Activity\n\n-   deploy api to connect\n\n-   put shiny frontend and call api\n\n## Testing and logging\n\n## Activity\n\n-   add logging and test to shiny app (if there's time)\n\n## Activity\n\n-   add app to server\n\n## \n\n",
    "supporting": [
      "workshop_full_slides_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}