[
  {
    "objectID": "supplemental_materials/linux_exercises_files/linux_exercises.html",
    "href": "supplemental_materials/linux_exercises_files/linux_exercises.html",
    "title": "Linux exercises",
    "section": "",
    "text": "Login to play with docker using your docker hub credentials.\nAdd an instance to your session."
  },
  {
    "objectID": "supplemental_materials/linux_exercises_files/linux_exercises.html#setup",
    "href": "supplemental_materials/linux_exercises_files/linux_exercises.html#setup",
    "title": "Linux exercises",
    "section": "",
    "text": "Login to play with docker using your docker hub credentials.\nAdd an instance to your session."
  },
  {
    "objectID": "supplemental_materials/linux_exercises_files/linux_exercises.html#exercises",
    "href": "supplemental_materials/linux_exercises_files/linux_exercises.html#exercises",
    "title": "Linux exercises",
    "section": "Exercises",
    "text": "Exercises\n\nIdentify the user that you are logged in as.\nwhoami\nIdentify the linux distribution of your instance.\ncat /etc/*-release\nUse a single command to list the directories that are on the server and save the list as a txt file called directories.txt. Use cat to inspect the text file.\n$ ls -la &gt; directories.txt\nExplore the directories and files using cd, pwd, and ls -la. What is missing in this Linux server based on this article?\nAlpine images tend to be quite small without all the files and directories that we need. Lets pull in a more recent Ubuntu linux image. Go to Docker Hub and find the command to pull the latest version of Ubuntu linux. Make sure to use the Docker Official Image. (We will be going into much more depth later on how Docker images and containers work).\n# https://hub.docker.com/_/ubuntu/tags\ndocker pull ubutu:latest\n\n# use this command to see what images have been pulled\ndocker image list \nRun the container interactively with docker run -it ubuntu and re-run exercises 1 through 4 to inspect the new directory structure.\nChange directories to your root directory and then create 2 folders titles test1 and test2.\ncd ~\nmkdir test1 test2\nCreate a new user with the adduser command and enter in the requested information. Identify which groups currently exist in the system. Add your new user to the root group and then switch to that new user.\nadduser rika\n\n# Adding user `rika' ...\n# Adding new group `rika' (1000) ...\n# Adding new user `rika' (1000) with group `rika' ...\n# Creating home directory `/home/rika' ...\n# Copying files from `/etc/skel' ...\n# New password: \n# Retype new password: \n# passwd: password updated successfully\n# Changing the user information for rika\n# Enter the new value, or press ENTER for the default\n       #  Full Name []: Rika\n       #  Room Number []: \n       #  Work Phone []: \n       # Home Phone []: \n       # Other []: \n# Is the information correct? [Y/n] Y\n\ngroups\nusermod -aG root rika \nsu rika\n# use ctrl + d to exit back to root\n\ncd between the root directory and the home directory of your new user to understand how user home directories are stored in a linux file system.\n# the tilde is a shortcut to the home directory of the signed in user\ncd ~ \npwd\ncd /\npwd\nLets do some updates for our server.\n# update packages\napt-get update\n# add sudo which temporarily elevates privileges allowing users to complete sensitive tasks without logging in as the root user\napt-get install sudo\n# add your user to the sudoers group\nusermod -aG sudo rika"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "DevOps for Data Scientists",
    "section": "",
    "text": "üìÖ September 17 and 18, 2023\n‚è∞ 09:00 - 17:00\nüíª https://posit-conf-2023.github.io/DevOps/\n\n\n\n\nThis workshop is intended for data scientists who wish to learn more about the basic principles and tools of DevOps and to get hands-on experience putting DevOps workflows into production.\n\n\n\nSection/Time\nTopics\nLabs\n\n\n\n\nPart 1\nWorkshop overview\nLogistics & setup\nIntroductions\nInfrastructure & wifi setup\n\n\nPart 2: DevOps Principles & Tools\nIntroduction to DevOps\nVersion control & github\nCI/CD\nReproducing workflows and environments\nLab #1: Deploy your own Quarto website on Github Pages & Posit Connect using GitHub Actions |\n\n\nPart 3: Docker for Data Scientists\nHow and why data scientists use docker in production\nDocker: overview and architecture\nBuilding docker images and containers\nPorts & networking\nLab #2: Write your own Dockerfile to deploy Open Source Shiny Server on Docker playground and host an app on the server\n\n\nPart 4: Data Science in Production\nChoosing your deployment method\nAPIs and when to use them\nJust enough auth\nLogging & metrics & testing\nLab #3: Host and secure an API on Posit Connect\n\n\nPart 5: Discussion\nCourse feedback\nQuestions for the team\nWorkshop Survey\n\n\n\n\n\n\nhttps://community.rstudio.com/t/devops-for-data-scientists-workshop-posit-conf-2023/171829\n\n\n\n\nSlides\nCode\nLab 1\nLab 2\nLab 3\n\n\n\n\n This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "DevOps for Data Scientists",
    "section": "",
    "text": "This workshop is intended for data scientists who wish to learn more about the basic principles and tools of DevOps and to get hands-on experience putting DevOps workflows into production.\n\n\n\nSection/Time\nTopics\nLabs\n\n\n\n\nPart 1\nWorkshop overview\nLogistics & setup\nIntroductions\nInfrastructure & wifi setup\n\n\nPart 2: DevOps Principles & Tools\nIntroduction to DevOps\nVersion control & github\nCI/CD\nReproducing workflows and environments\nLab #1: Deploy your own Quarto website on Github Pages & Posit Connect using GitHub Actions |\n\n\nPart 3: Docker for Data Scientists\nHow and why data scientists use docker in production\nDocker: overview and architecture\nBuilding docker images and containers\nPorts & networking\nLab #2: Write your own Dockerfile to deploy Open Source Shiny Server on Docker playground and host an app on the server\n\n\nPart 4: Data Science in Production\nChoosing your deployment method\nAPIs and when to use them\nJust enough auth\nLogging & metrics & testing\nLab #3: Host and secure an API on Posit Connect\n\n\nPart 5: Discussion\nCourse feedback\nQuestions for the team\nWorkshop Survey"
  },
  {
    "objectID": "index.html#pre-work",
    "href": "index.html#pre-work",
    "title": "DevOps for Data Scientists",
    "section": "",
    "text": "https://community.rstudio.com/t/devops-for-data-scientists-workshop-posit-conf-2023/171829"
  },
  {
    "objectID": "index.html#materials",
    "href": "index.html#materials",
    "title": "DevOps for Data Scientists",
    "section": "",
    "text": "Slides\nCode\nLab 1\nLab 2\nLab 3"
  },
  {
    "objectID": "index.html#section",
    "href": "index.html#section",
    "title": "DevOps for Data Scientists",
    "section": "",
    "text": "This work is licensed under a Creative Commons Attribution 4.0 International License."
  },
  {
    "objectID": "coursework_labs/02_lab_docker/Exercises.html",
    "href": "coursework_labs/02_lab_docker/Exercises.html",
    "title": "Lab: Docker",
    "section": "",
    "text": "Create a dockerhub username and password\n\nUsing your Docker hub username and password, login to https://labs.play-with-docker.com/\nClick + ADD NEW INSTANCE\nCongratulations! You are now in an Alpine Linux instance directly on your browser. Check this by running cat /etc/*-release in the command line interface.\n\n\n\n\n\n\n\nNote\n\n\n\nYour terminal should say something like [node1] (local) root@123.123.0.12 If it doesn‚Äôt refresh your screen or add another instance.\n\n\n\n\n\nThe basic docker run command takes this form:\ndocker run [OPTIONS] [IMAGE:TAG] [COMMAND] [ARG...]\nIn the below exercise we will practice running docker containers with different options or ‚Äúflags.‚Äù\n\nCurrently we have no docker images downloaded. Confirm this with docker image ls -a.\n\nPull down a Dockerhub linux image. Confirm that the image is downloaded with the ls command.\n\ndocker pull ubuntu\ndocker image ls -a\n\nRun an interactive container with the bash shell attached. Run a few linux commands to explore your environment and then exit the container.\n\ndocker run -it ubuntu bash\nls\nwhoami\nhostname\n# exit the container with Ctrl+D or exit\nThis runs the container in the foreground so you are unable to access the command prompt for your original alpine server. For this reason interactive mode is often used for development and testing.\n\nRun the container in detached mode and then list all your containers.\n\ndocker run -d ubuntu\ndocker container ls -a\nYou should see that the ubuntu container was created and then exited. The container ID is shown with an exited status and the command line is still accessible.\nDetached containers run in the background, so the container keeps running until the application process exits (which is what happened here), or you stop the container. For this reason detached mode is often used for production purposes.\n\nRun an nginx web server in detached mode to see what happens when the process doesnt just exit. The image will be automatically pulled from Dockerhub if it is not found locally so there is no need to run docker pull first.\n\ndocker run -d -P --name nginx1 nginx:alpine\n\n# -P publishes network ports so you can send traffic into the container\n# --name gives the container a name so you can work with it in other commands\nClick on the port button at the top of your page and enter 32768.\n\nThis should bring you to the nginx server.\n\n\nExamine your container and then stop it.\n\n# check your running containers\ndocker container ls -a\n\n# you can also check your running processes\ndocker ps -a\n\n# stop the container using its name\ndocker container stop nginx1\n\nRun a container with a different linux distro and then automatically remove it. Add an echo command to confirm that the container has actually run.\n\ndocker run --rm debian echo \"hello world\"\nThis mode is usually used on foreground containers that perform short-term tasks such as tests or database backups. Since a container is ephemeral, once it is removed anything you may have downloaded or created in the container is also destroyed.\nCheck to see that the container was completely removed. You shouldnt see the debian container in the output at all.\ndocker container ls -a\n\n\n\nThe docker exec command is very similar to the docker run -it command. Both are very helpful for debugging containers as they allow you to jump inside your container instance. The exec command needs a running container to execute any command, whereas the -it flag starts a container and places you into a terminal in interactive mode. Use the docker exec command to execute a bash command in a running container. This can be used to execute any command within a running container.\nBe careful not use docker exec to change your container as once it is deleted you will lose any changes you‚Äôve made!\ndocker exec requires two arguments - the container and the command you want to run.\ndocker exec [OPTIONS] CONTAINER [COMMAND] [ARG...]\n\nUse docker run -it to jump into an ubuntu container.\n\ndocker run -it ubuntu\nexit\n\nUse docker exec to run commands in a container\n\ndocker container ls -a # to get a container ID of a running container\ndocker exec -it CONTAINER_ID bash\nexit\ndocker exec CONTAINER_ID ls \n\nLets run a detached MySQL container and then check out some logs. The database requires a password to work.In production you should never pass credentials directly in your command but we will do it for testing purposes. (The forward slashes below allow you to use a new line for your code)\n\n docker container run -d --name mydb \\\n -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ \n mysql\n \ndocker container logs mydb\n\n\n\n\nPull two versions of the same image\n\ndocker pull httpd:alpine\ndocker pull httpd:latest\n\nInspect ports.\n\ndocker inspect  httpd:latest\ndocker inspect  httpd:alpine\n\nMap two different host ports to the same application port for the two containers.\n\ndocker run -d -p 3456:80 --name httpd-latest httpd:latest\ndocker run -d -p 80:80 --name httpd-alpine httpd:alpine\ndocker ps\n\n\n\n\nStop your running containers with docker stop and return to your original command prompt.\nCreate a text file.\n\ntouch test.txt # create a text file\necho \"this is a test file\" &gt; test.txt # redirect string to the file\ncat test.txt # confirm that the echo command worked\n\nWe want to add this file from our host machine into a separate Ubuntu container.\n\npwd # to see where the test.txt file is located on your host machine\n\ndocker run -it -d -v /root:/data ubuntu # mount a shared volume to a folder in the container called \"data\"\n\ndocker exec -it CONTAINER_ID bash\n\nls # see what folders are in the container\n\ncd data # move to the newly created data directory\n\nls # confirm that the text file is there\nIn order to get data in or out of a container, you need to mount a shared volume (directory) between the container and host with the -v flag. You specify a host directory and a container directory separated by :. Anything in the volume will be available to both the host and the container at the file paths specified.\n\n\n\n\nExit out of your container from the previous exercise with exit and confirm that the container is still running. This container should have the new directory and text file.\nRun docker diff CONTAINER_ID to see the difference between the base image and the new container. You should see the new data folder,\nLogin to docker hub in your instance using docker login and enter your username and password.\nCommit the changed ubuntu image and give it a new name like ubuntu_text.\n\ndocker commit CONTAINER_ID ubuntu_text\ndocker image ls # to check the new image is available\n\nTag and push the image to dockerhub. Login to docker hub to see your saved and shareable image!\n\ndocker tag ubuntu_text docker_hub_username/ubuntu_text\ndocker push docker_hub_username/ubuntu_text\n\n\n\nBest practice is to create a Dockerfile so that any changes to your image can be documented.\n\nCreate a Dockerfile (no file extension needed)in your instance with touch Dockerfile and add the following to it.\n\ntouch Dockerfile\nvim Dockerfile\n# press i to enter insert mode\n\nFROM rocker/shiny:4.3.1\nCMD [\"/usr/bin/shiny-server\"]\n\n# press escape \n:wq # to save and exit\n\nBuild the image with docker build -t my_server . where my_server is the name of your new image\nRun your container with docker run -d -p 3838:3838 my_server\n\nthe -p flag maps a port from the host to a port in the container.\nthis gives us the ability to access the services running inside the container\nexample: `-p 8080:80` maps port 8080 on the host to port 80 in the container.\nin our code we use port 3838 because that is the port that Shiny Server uses by default.\n\nA port number will appear - click on it to access the home page of Shiny Server!\n\n\n\nYou should see something like this:\n\nWe want to change the home page and have it show an app of our choosing instead. Exec into your container and let‚Äôs find where the information for the home page is stored.\n\n# get the container ID or name \ndocker container ls \n\n# execute into the bash shell in your container\ndocker exec -it CONTAINER_ID bash \n\n# list your folders\nls \n\n#this will show you all the server executables for basic shell commands. See if you can find those you've used like touch\ncd usr/bin \nls\n\n# look at the code for the home page,. Notice where the sample \"It's Alive\" and \"Shiny Doc\" apps on the home page are being pulled from\ncd /srv/shiny-server \nls\ncat index.html \n\n# look at the configuration file for the server\ncd /etc/shiny-server\ncat shiny-server.conf \n\nLet‚Äôs delete the home page and the sample apps in the server.\n\nsudo rm /srv/shiny-server/index.html\nsudo rm -rf /srv/shiny-server/sample-apps\n# refresh your shiny server webpage\nNotice how the server home page now has a directory of the apps that live in /srv/shiny-server. Recall how the configuration file had directory_index turned on.\n\nLet‚Äôs create a basic shiny app and then rebuild our image. We will move our app to the server in our Dockerfile instead of on the fly in our run command.\n\nexit\ndocker container ls \ndocker container stop CONTAINER_IT \nmkdir apps\ncd apps\ntouch app.R\nvim app.R\n# press i to insert the following code\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n# press escape \n:wq to save \n\n##docker run -d -p 3838:3838 -v ./apps:/srv/shiny-server my_server # my_server is the name of the docker image that you built\n\nUpdate our Dockerfile\n\ncd /root\nvim Dockerfile\n# press i to insert code\n\nFROM rocker/shiny:4.3.1\n# comes preinstalled with a bunch of packages\n\nRUN apt-get update && apt-get install -y \\\n    libcurl4-gnutls-dev \\\n    libssl-dev\n\nRUN R -e 'install.packages(c(\\\n              \"shiny\", \\\n              \"palmerpenguins\", \\\n              \"shinydashboard\", \\\n              \"ggplot2\" \\\n            ), \\\n            repos=\"https://packagemanager.rstudio.com/cran/__linux__/jammy\"\\\n          )'\n          \nCOPY ./apps/* /srv/shiny-server/\n\nCMD [\"/usr/bin/shiny-server\"]\n\nRebuild the dockerfile with a new name\n\ndocker build -t new_app .\n\nRun the container. Make sure to stop the previous container first as it was already using the port 3838.\n\ndocker run -d -p 3838:3838 new_app"
  },
  {
    "objectID": "coursework_labs/02_lab_docker/Exercises.html#goals",
    "href": "coursework_labs/02_lab_docker/Exercises.html#goals",
    "title": "Lab: Docker",
    "section": "",
    "text": "Create a dockerhub username and password\n\nUsing your Docker hub username and password, login to https://labs.play-with-docker.com/\nClick + ADD NEW INSTANCE\nCongratulations! You are now in an Alpine Linux instance directly on your browser. Check this by running cat /etc/*-release in the command line interface.\n\n\n\n\n\n\n\nNote\n\n\n\nYour terminal should say something like [node1] (local) root@123.123.0.12 If it doesn‚Äôt refresh your screen or add another instance.\n\n\n\n\n\nThe basic docker run command takes this form:\ndocker run [OPTIONS] [IMAGE:TAG] [COMMAND] [ARG...]\nIn the below exercise we will practice running docker containers with different options or ‚Äúflags.‚Äù\n\nCurrently we have no docker images downloaded. Confirm this with docker image ls -a.\n\nPull down a Dockerhub linux image. Confirm that the image is downloaded with the ls command.\n\ndocker pull ubuntu\ndocker image ls -a\n\nRun an interactive container with the bash shell attached. Run a few linux commands to explore your environment and then exit the container.\n\ndocker run -it ubuntu bash\nls\nwhoami\nhostname\n# exit the container with Ctrl+D or exit\nThis runs the container in the foreground so you are unable to access the command prompt for your original alpine server. For this reason interactive mode is often used for development and testing.\n\nRun the container in detached mode and then list all your containers.\n\ndocker run -d ubuntu\ndocker container ls -a\nYou should see that the ubuntu container was created and then exited. The container ID is shown with an exited status and the command line is still accessible.\nDetached containers run in the background, so the container keeps running until the application process exits (which is what happened here), or you stop the container. For this reason detached mode is often used for production purposes.\n\nRun an nginx web server in detached mode to see what happens when the process doesnt just exit. The image will be automatically pulled from Dockerhub if it is not found locally so there is no need to run docker pull first.\n\ndocker run -d -P --name nginx1 nginx:alpine\n\n# -P publishes network ports so you can send traffic into the container\n# --name gives the container a name so you can work with it in other commands\nClick on the port button at the top of your page and enter 32768.\n\nThis should bring you to the nginx server.\n\n\nExamine your container and then stop it.\n\n# check your running containers\ndocker container ls -a\n\n# you can also check your running processes\ndocker ps -a\n\n# stop the container using its name\ndocker container stop nginx1\n\nRun a container with a different linux distro and then automatically remove it. Add an echo command to confirm that the container has actually run.\n\ndocker run --rm debian echo \"hello world\"\nThis mode is usually used on foreground containers that perform short-term tasks such as tests or database backups. Since a container is ephemeral, once it is removed anything you may have downloaded or created in the container is also destroyed.\nCheck to see that the container was completely removed. You shouldnt see the debian container in the output at all.\ndocker container ls -a\n\n\n\nThe docker exec command is very similar to the docker run -it command. Both are very helpful for debugging containers as they allow you to jump inside your container instance. The exec command needs a running container to execute any command, whereas the -it flag starts a container and places you into a terminal in interactive mode. Use the docker exec command to execute a bash command in a running container. This can be used to execute any command within a running container.\nBe careful not use docker exec to change your container as once it is deleted you will lose any changes you‚Äôve made!\ndocker exec requires two arguments - the container and the command you want to run.\ndocker exec [OPTIONS] CONTAINER [COMMAND] [ARG...]\n\nUse docker run -it to jump into an ubuntu container.\n\ndocker run -it ubuntu\nexit\n\nUse docker exec to run commands in a container\n\ndocker container ls -a # to get a container ID of a running container\ndocker exec -it CONTAINER_ID bash\nexit\ndocker exec CONTAINER_ID ls \n\nLets run a detached MySQL container and then check out some logs. The database requires a password to work.In production you should never pass credentials directly in your command but we will do it for testing purposes. (The forward slashes below allow you to use a new line for your code)\n\n docker container run -d --name mydb \\\n -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ \n mysql\n \ndocker container logs mydb\n\n\n\n\nPull two versions of the same image\n\ndocker pull httpd:alpine\ndocker pull httpd:latest\n\nInspect ports.\n\ndocker inspect  httpd:latest\ndocker inspect  httpd:alpine\n\nMap two different host ports to the same application port for the two containers.\n\ndocker run -d -p 3456:80 --name httpd-latest httpd:latest\ndocker run -d -p 80:80 --name httpd-alpine httpd:alpine\ndocker ps\n\n\n\n\nStop your running containers with docker stop and return to your original command prompt.\nCreate a text file.\n\ntouch test.txt # create a text file\necho \"this is a test file\" &gt; test.txt # redirect string to the file\ncat test.txt # confirm that the echo command worked\n\nWe want to add this file from our host machine into a separate Ubuntu container.\n\npwd # to see where the test.txt file is located on your host machine\n\ndocker run -it -d -v /root:/data ubuntu # mount a shared volume to a folder in the container called \"data\"\n\ndocker exec -it CONTAINER_ID bash\n\nls # see what folders are in the container\n\ncd data # move to the newly created data directory\n\nls # confirm that the text file is there\nIn order to get data in or out of a container, you need to mount a shared volume (directory) between the container and host with the -v flag. You specify a host directory and a container directory separated by :. Anything in the volume will be available to both the host and the container at the file paths specified.\n\n\n\n\nExit out of your container from the previous exercise with exit and confirm that the container is still running. This container should have the new directory and text file.\nRun docker diff CONTAINER_ID to see the difference between the base image and the new container. You should see the new data folder,\nLogin to docker hub in your instance using docker login and enter your username and password.\nCommit the changed ubuntu image and give it a new name like ubuntu_text.\n\ndocker commit CONTAINER_ID ubuntu_text\ndocker image ls # to check the new image is available\n\nTag and push the image to dockerhub. Login to docker hub to see your saved and shareable image!\n\ndocker tag ubuntu_text docker_hub_username/ubuntu_text\ndocker push docker_hub_username/ubuntu_text\n\n\n\nBest practice is to create a Dockerfile so that any changes to your image can be documented.\n\nCreate a Dockerfile (no file extension needed)in your instance with touch Dockerfile and add the following to it.\n\ntouch Dockerfile\nvim Dockerfile\n# press i to enter insert mode\n\nFROM rocker/shiny:4.3.1\nCMD [\"/usr/bin/shiny-server\"]\n\n# press escape \n:wq # to save and exit\n\nBuild the image with docker build -t my_server . where my_server is the name of your new image\nRun your container with docker run -d -p 3838:3838 my_server\n\nthe -p flag maps a port from the host to a port in the container.\nthis gives us the ability to access the services running inside the container\nexample: `-p 8080:80` maps port 8080 on the host to port 80 in the container.\nin our code we use port 3838 because that is the port that Shiny Server uses by default.\n\nA port number will appear - click on it to access the home page of Shiny Server!\n\n\n\nYou should see something like this:\n\nWe want to change the home page and have it show an app of our choosing instead. Exec into your container and let‚Äôs find where the information for the home page is stored.\n\n# get the container ID or name \ndocker container ls \n\n# execute into the bash shell in your container\ndocker exec -it CONTAINER_ID bash \n\n# list your folders\nls \n\n#this will show you all the server executables for basic shell commands. See if you can find those you've used like touch\ncd usr/bin \nls\n\n# look at the code for the home page,. Notice where the sample \"It's Alive\" and \"Shiny Doc\" apps on the home page are being pulled from\ncd /srv/shiny-server \nls\ncat index.html \n\n# look at the configuration file for the server\ncd /etc/shiny-server\ncat shiny-server.conf \n\nLet‚Äôs delete the home page and the sample apps in the server.\n\nsudo rm /srv/shiny-server/index.html\nsudo rm -rf /srv/shiny-server/sample-apps\n# refresh your shiny server webpage\nNotice how the server home page now has a directory of the apps that live in /srv/shiny-server. Recall how the configuration file had directory_index turned on.\n\nLet‚Äôs create a basic shiny app and then rebuild our image. We will move our app to the server in our Dockerfile instead of on the fly in our run command.\n\nexit\ndocker container ls \ndocker container stop CONTAINER_IT \nmkdir apps\ncd apps\ntouch app.R\nvim app.R\n# press i to insert the following code\n\nlibrary(shiny)\n\n# Define UI for application that draws a histogram\nui &lt;- fluidPage(\n\n    # Application title\n    titlePanel(\"Old Faithful Geyser Data\"),\n\n    # Sidebar with a slider input for number of bins \n    sidebarLayout(\n        sidebarPanel(\n            sliderInput(\"bins\",\n                        \"Number of bins:\",\n                        min = 1,\n                        max = 50,\n                        value = 30)\n        ),\n\n        # Show a plot of the generated distribution\n        mainPanel(\n           plotOutput(\"distPlot\")\n        )\n    )\n)\n\n# Define server logic required to draw a histogram\nserver &lt;- function(input, output) {\n\n    output$distPlot &lt;- renderPlot({\n        # generate bins based on input$bins from ui.R\n        x    &lt;- faithful[, 2]\n        bins &lt;- seq(min(x), max(x), length.out = input$bins + 1)\n\n        # draw the histogram with the specified number of bins\n        hist(x, breaks = bins, col = 'darkgray', border = 'white',\n             xlab = 'Waiting time to next eruption (in mins)',\n             main = 'Histogram of waiting times')\n    })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)\n\n# press escape \n:wq to save \n\n##docker run -d -p 3838:3838 -v ./apps:/srv/shiny-server my_server # my_server is the name of the docker image that you built\n\nUpdate our Dockerfile\n\ncd /root\nvim Dockerfile\n# press i to insert code\n\nFROM rocker/shiny:4.3.1\n# comes preinstalled with a bunch of packages\n\nRUN apt-get update && apt-get install -y \\\n    libcurl4-gnutls-dev \\\n    libssl-dev\n\nRUN R -e 'install.packages(c(\\\n              \"shiny\", \\\n              \"palmerpenguins\", \\\n              \"shinydashboard\", \\\n              \"ggplot2\" \\\n            ), \\\n            repos=\"https://packagemanager.rstudio.com/cran/__linux__/jammy\"\\\n          )'\n          \nCOPY ./apps/* /srv/shiny-server/\n\nCMD [\"/usr/bin/shiny-server\"]\n\nRebuild the dockerfile with a new name\n\ndocker build -t new_app .\n\nRun the container. Make sure to stop the previous container first as it was already using the port 3838.\n\ndocker run -d -p 3838:3838 new_app"
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "",
    "text": "To get familiar with how the renv package helps create reproducible environments for your R projects.\nTo get comfortable using the terminal for interacting with git and github.\nTo understand how to authenticate to github using HTTPS.\nTo practice writing and reading a yaml file.\nTo understand the basics of a quarto website.\nTo deploy a quarto site to Github Pages using Github Actions for Continuous Deployment. This will allow you to update your site every time you push a commit to git."
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#goals",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#goals",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "",
    "text": "To get familiar with how the renv package helps create reproducible environments for your R projects.\nTo get comfortable using the terminal for interacting with git and github.\nTo understand how to authenticate to github using HTTPS.\nTo practice writing and reading a yaml file.\nTo understand the basics of a quarto website.\nTo deploy a quarto site to Github Pages using Github Actions for Continuous Deployment. This will allow you to update your site every time you push a commit to git."
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#setup",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#setup",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "Setup",
    "text": "Setup\n\nTo setup your cloud-based development environment create an account using your email at http://rstd.io/class with code devops_workshop. Click on the Rstudio Workbench widget to start your environment.\nClick on + New Session to create a new Rstudio Pro session.\n\n\n\n\n\n\n\nNote\n\n\n\nYou can also set up your project in your local IDE but then you will need to install Quarto and other environment dependencies."
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-1-create-a-local-quarto-website",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-1-create-a-local-quarto-website",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "Part 1: Create a local Quarto website",
    "text": "Part 1: Create a local Quarto website\nQuarto is a multi-lingual open-source scientific and technical publishing system that lets you create documents, articles, blogs, websites, and more.\n\nOpen a session in your workbench environment. Give it a name.\nTo create a new website project within RStudio, click on File &gt; New Project &gt; New Directory &gt; Quarto Website and save it to a directory with a name.\n\n\n\n\n\n\n\nquarto site file structure\n\n\n\n\n\n_quarto.yml - project file\nabout.qmd - About page\nindex.qmd - Home page\nstyles.css - custom CSS\n_site - this will be created after you render your files\n\n\n\n\nPress render to see your new website! Or preview via terminal with quarto preview. Make sure that your web browser allows popups. This may take a few moments to install some required packages.\nCreate a repository on github but do not initialize it with a README, license, or gitignore files. This repo will host the files for the site that you will be deploying to Github Pages.\n\nName your repository username.github.io where username is your github username. If you already have this name in use, then name your repo quarto_website\n\nOpen the terminal tab in your Rstudio Workbench environment, cd into your project directory.\nInitialize a local git repository with git init.\nRename your master branch to main so that branch names are consistent across your github repo and your local git. You can use git branch to see what branches exist on your repo and switch between them using git checkout &lt;branch&gt;.\n\n# in your terminal\ngit branch -m master main\n\nConfigure your github information in the terminal. To set your global commit name and email address run the git config command with the --global option:\ngit config --global user.name \"Your Name\"\ngit config --global user.email \"youremail@yourdomain.com\"\ngit config --global init.defaultBranch main\nOnce done, you can confirm that the information is set by running:\ngit config --list\nCreate a .gitignore file in your main directory. Add the following to the file and save:\n\n/.quarto/\n/_site/\n.Rproj.user\n.Rhistory"
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-2-renv-workflow",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-2-renv-workflow",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "Part 2: renv workflow",
    "text": "Part 2: renv workflow\n\nExercises:\n\nRun .libPaths() to see your library path\nRun lapply(.libPaths(), list.files) to see what is in your library directory\n\nInstall and initialize an renv workflow.\n\n\n\n\n\n\n\nrenv workflow\n\n\n\n\n\n\nInstall the renv package with install.packages(\"renv\")\nRun renv::init() in your console to initialize a new project-local environment with a private R library and click yes to the prompt.\nChange the title of your site in index.qmd and try adding a library. After you‚Äôve done some work on your code take a snapshot with renv::snapshot()\nRun .libPaths() and lapply(.libPaths(), list.files)again. What has changed?\nRecreate your environment when collaborating or coming back to your work with renv::restore()\n\n\n\n\n\n(Optional) Learning how to use all the power of Quarto is beyond the scope of this workshop but you can use the below resources to play around with difference templates and designs for your new website.\n\n\n\nhttps://rstudio-conf-2022.github.io/get-started-quarto/materials/06-websites.html#/websites\nhttps://quarto.org/docs/gallery/#websites\nhttps://quarto.org/docs/websites/website-blog.html#themes\nhttps://www.marvinschmitt.com/blog/website-tutorial-quarto/\nhttps://quarto.org/docs/websites/#workflow"
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-3-version-control-and-authentication-with-github",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-3-version-control-and-authentication-with-github",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "Part 3: Version Control and Authentication with Github",
    "text": "Part 3: Version Control and Authentication with Github\n\nWe will need to securely authenticate from our server environment to the server that lives on Github. You can do this via two different authentication mechanisms - SSH or HTTPS. Note that there are two urls created for your repo on github- one is for https and the other is for SSH.\n\n\nHTTPSSSH\n\n\nAuthentication:\n\nCopy the https repository URL that you created.\nCreate a classic personal access token and give it permissions to your repository for ‚Äúrepo‚Äù, ‚Äúuser‚Äù, and ‚Äúworkflow‚Äù. Make sure to note your token somewhere safe.\nInstall usethis with install.packages(\"usethis\")\nSave your PAT with\ngitcreds::gitcreds_set()\n\nVersion Control:\n\nAdd your files to local version control with git add . in the terminal.\nCommit your files with git commit -m \"my first commit\"\nUsing the https url from github add your github repository as the ‚Äúremote‚Äù with git remote add origin https://github.com/username/repo_name.git\nPush your local files to github with git push -u origin main where main is the name of your branch in github. You will be prompted for a username and password. Enter your github username and the personal access token from above as your password.\n\nResources: https://docs.github.com/en/migrations/importing-source-code/using-the-command-line-to-import-source-code/adding-locally-hosted-code-to-github\n\n\nAuthentication:\n\nCopy the SSH repository URL that you created.\nGenerate a new SSH key on your server - open the terminal tab in your Rstudio Workbench environment and type ssh-keygen -t ed25519 -C \"your_github_email@example.com\" . Press enter 3 times to leave the file path as the default and to leave the passphrase blank.\nStart the ssh-agent with eval \"$(ssh-agent -s)\". The ssh-agent is a key manager for SSH. It holds your keys and certificates in memory, un-encrypted, and ready for use by ssh.\nCreate a new hidden file (dot files and directories are hidden but you can see them with ls -la) with touch ~/.ssh/config .\nEdit the file using vi or vim and then add your SSH private key to the ssh-agent. Use the cheat sheet below for common vim commands.\n\nvim touch ~/.ssh/config\ni # to get into insert mode\nHost github.com\n  IgnoreUnknown UseKeychain\n  AddKeysToAgent yes\n  UseKeychain yes\n  IdentityFile ~/.ssh/id_ed25519\n# Press the escape key to exit insert mode\n:wq # to save and exit out of the file\nssh-add ~/.ssh/id_ed25519\n\nCopy your public key with cat ~/.ssh/id_ed25519.pub\nIn your Github account go to Settings &gt; SSH and GPG Keys, and add your SSH key. Give it a name like ‚Äúworkbench key.‚Äù\n\nVersion Control:\n\nAdd your files to local version control with git add .\nCommit your files with git commit -m \"my first commit\"\nUsing the https url from github add your github repository as the ‚Äúremote‚Äù with git remote add origin git@github.com:username/repo_name.git\nPush your local files to github with git push -u origin main where main is the name of your branch in github. Type yes to add github to your list of known hosts.\n\nResources\n\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/generating-a-new-ssh-key-and-adding-it-to-the-ssh-agent\nhttps://docs.github.com/en/authentication/connecting-to-github-with-ssh/adding-a-new-ssh-key-to-your-github-account?platform=linux\nVim Commands Cheat Sheet\n\n\n\n\n\n\n\n\n\n\nGithub CLI\n\n\n\nTo learn more about the Github CLI and the gh package please check out this article."
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-4-publish-using-gha",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-4-publish-using-gha",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "Part 4: Publish using GHA",
    "text": "Part 4: Publish using GHA\n\nWe need to create a gh-pages branch to publish to github pages.\n\ngit status # - make sure everything is committed! \ngit checkout --orphan gh-pages\ngit reset --hard # make sure you've committed changes before running this!\ngit commit --allow-empty -m \"Initialising gh-pages branch\"\ngit push origin gh-pages\ngit checkout main\n\nGo to Settings &gt; Pages in your github repository and make sure that your site is being built from the gh-pages branch.\n\n\n\nIn your terminal run quarto publish gh-pages and click Yes to the prompt to update the site.\nYour site should now be live!\nCreate a github action called publish.yaml and save it in .github/workflows:\n\nmkdir .github\ncd .github\nmkdir workflows\ncd workflows\ntouch publish.yaml\n\nEdit the publish.yaml file:\n\nname: Deploy quarto site to Github Pages\n\non:\n  workflow_dispatch:\n  push:\n    branches: ['main']\n\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: '4.2.3'\n          use-public-rspm: true\n\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n        env:\n          RENV_CONFIG_REPOS_OVERRIDE: https://packagemanager.rstudio.com/all/latest\n\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n\nSnapshot your renv environment with renv::snapshot.\nCommit and push your code: (Make sure that your renv.lock file has been added!)\n\ngit add .\ngit commit -m \"created action\"\ngit push\n\nGo to the actions tab in your Github repository and watch your deploy!"
  },
  {
    "objectID": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-5-publish-to-connect-with-gha",
    "href": "coursework_labs/01_lab_devops_CICD/Deploy_Quarto_GHA.html#part-5-publish-to-connect-with-gha",
    "title": "Lab: Deploy Quarto with GHA",
    "section": "Part 5: Publish to Connect with GHA",
    "text": "Part 5: Publish to Connect with GHA\n\nCreate an API key on the Connect website. Note this API somewhere safe as you will be using it multiple times in this workshop.\n\n\n\nAdd the Connect URL and your API key to your Github repository:\n\n\nIn your repository go to Settings &gt; Security &gt; Secrets and variables &gt; Actions\nAdd 2 secrets:\n\n\n\nThe rsconnect package needs to be explicitly called in your code to be correctly snapshot in your lock file. Install it and call it in one of your files.\n\ninstall.packages(\"rsconnect\")\nlibrary(rsconnect)\nrenv::snapshot()\n\nEdit your publish.yaml file:\n\n---\non:\n  workflow_dispatch:\n  push:\n    branches:\n      - main\nname: Quarto and Connect Publish\njobs:\n  build-deploy:\n    runs-on: ubuntu-latest\n    permissions:\n      contents: write\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: 4.2.3\n          use-public-rspm: true\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n        with:\n          cache-version: 1\n      - name: Render and Publish\n        uses: quarto-dev/quarto-actions/publish@v2\n        with:\n          target: gh-pages\n        env:\n          GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }}\n  test-and-connect-publish:\n    runs-on: ubuntu-latest\n    steps:\n      - name: Check out repository\n        uses: actions/checkout@v4\n      - name: Set up Quarto\n        uses: quarto-dev/quarto-actions/setup@v2\n      - name: Install R\n        uses: r-lib/actions/setup-r@v2\n        with:\n          r-version: 4.2.3\n          use-public-rspm: true\n      - name: Setup renv and install packages\n        uses: r-lib/actions/setup-renv@v2\n      - name: Create manifest.json\n        shell: Rscript {0}\n        run: |\n          rsconnect::writeManifest()\n      - name: Publish Connect content\n        uses: rstudio/actions/connect-publish@main\n        with:\n          url: ${{ secrets.CONNECT_SERVER }}\n          api-key: ${{ secrets.CONNECT_API_KEY }}\n          access-type: logged_in\n          dir: |\n            .:/github-actions"
  },
  {
    "objectID": "coursework_labs/03_lab_data_in_prod/lab_solution.html",
    "href": "coursework_labs/03_lab_data_in_prod/lab_solution.html",
    "title": "Lab 3: Data in Production",
    "section": "",
    "text": "To setup your cloud-based development environment create an account using your email at http://rstd.io/class with code devops_workshop. Click on the Rstudio Connect widget to start your environment."
  },
  {
    "objectID": "coursework_labs/03_lab_data_in_prod/lab_solution.html#setup",
    "href": "coursework_labs/03_lab_data_in_prod/lab_solution.html#setup",
    "title": "Lab 3: Data in Production",
    "section": "",
    "text": "To setup your cloud-based development environment create an account using your email at http://rstd.io/class with code devops_workshop. Click on the Rstudio Connect widget to start your environment."
  },
  {
    "objectID": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-1-host-api-on-posit-connect",
    "href": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-1-host-api-on-posit-connect",
    "title": "Lab 3: Data in Production",
    "section": "Part 1: Host API on Posit Connect",
    "text": "Part 1: Host API on Posit Connect\n\nGo to the Solutions Engineering R-examples repo and copy the HTTPS url.\nIn Connect click Publish and Import from Git. Enter the URL that you copied above and click Next.\nSelect the main branch and click next.\nSelect the plumber-penguins/app directory and give it a name.\nClick `Deploy Content. In a few moments your API should be live!"
  },
  {
    "objectID": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-2-explore-your-api",
    "href": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-2-explore-your-api",
    "title": "Lab 3: Data in Production",
    "section": "Part 2: Explore your API",
    "text": "Part 2: Explore your API\n\nExplore the endpoints for the API. Click the GET for each endpoint and then click Try it out and Execute.\nClick on the link ending in openapi.json below the title of you API.\n\n\n\nTry accessing the API via the terminal with curl &lt;URL&gt;. You should receive output that the app is not authorized!\nAuthorize the app for everyone by changing the Access &gt; Sharing option to Anyone - no login required and then try the curl command again. It should work now!\n\n\n\nTry out the /penguins endpoint and grab the Request URL.\nAccess the /penguins endpoint and provide input for the sample size in your terminal and in the app itself.\n\ncurl \"&lt;REQUEST URL&gt;/penguins?sample_size=5\""
  },
  {
    "objectID": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-3-plumber-examples",
    "href": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-3-plumber-examples",
    "title": "Lab 3: Data in Production",
    "section": "Part 3: Plumber Examples",
    "text": "Part 3: Plumber Examples\n\nInstall the plumber examples package. remotes::install_github(\"sol-eng/plumberExamples\")\nRun available_apis to see plumber examples in the package.\nAccess examples and code:\n\nlibrary(plumber)\nplumb_api(package = \"plumberExamples\", name = \"00-hello\") %&gt;% pr_run()"
  },
  {
    "objectID": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-4-push-button-deployment",
    "href": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-4-push-button-deployment",
    "title": "Lab 3: Data in Production",
    "section": "Part 4: Push-button deployment",
    "text": "Part 4: Push-button deployment\n\nPublish the 11-car-inventory example to Connect.\nClick the blue publishing icon in the upper right of the file editor.\n\n\n\nWhen prompted connect to the Posit Connect server with the provided url. Click Publish."
  },
  {
    "objectID": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-5-programmatically-access-connect",
    "href": "coursework_labs/03_lab_data_in_prod/lab_solution.html#part-5-programmatically-access-connect",
    "title": "Lab 3: Data in Production",
    "section": "Part 5: Programmatically access Connect",
    "text": "Part 5: Programmatically access Connect\n\nWe want to programmatically identify all the content that has been published. You will need to use the Connect API Key that you created in the earlier lab. Or you can create a new key.\n\n\n\nOpen a new R script In your workbench console. Add your Connect information:\n\n# Add server\nrsconnect::addServer(\n  url = \"https://liberal-bullfinch.74633.fleeting.rstd.io/rsconnect/__api__\",\n  name = \"colorado\"\n)\n\n# Add account\nrsconnect::connectApiUser(\n  account = \"\",\n  server = \"colorado\",\n  apiKey = Sys.getenv(\"CONNECT_API_KEY\"),\n)\n\nAccess your content programmatically in R: - need to add key ias env variable\n\nlibrary(httr)\nlibrary(tidyr)\n\n# Use the /v1/content endpoint to retrieve the full list of content items\nresult &lt;- GET(\n  paste0(Sys.getenv(\"CONNECT_SERVER\"),\"__api__/v1/content\"),\n    add_headers(Authorization = paste(\"Key\", Sys.getenv(\"CONNECT_API_KEY\"))))\n\n# Create a tibble for the content list result response\ndf_full &lt;- unnest_wider(tibble::tibble(dat = content(result)), dat) \n\nAccess your content programmatically in the terminal using curl:\n\nexport CONNECT_API_KEY=XXX\nexport CONNECT_SERVER=https://liberal-bullfinch.74633.fleeting.rstd.io/rsconnect/\n\ncurl --silent --show-error -L --max-redirs 0 --fail \\\n    -H \"Authorization: Key ${CONNECT_API_KEY}\" \\\n    \"${CONNECT_SERVER}__api__/v1/content\""
  },
  {
    "objectID": "slides/workshop_full_slides.html#part-1-introductions-setup-workshop-overview",
    "href": "slides/workshop_full_slides.html#part-1-introductions-setup-workshop-overview",
    "title": "DevOps for Data Scientists",
    "section": "Part 1: Introductions, setup, & workshop overview",
    "text": "Part 1: Introductions, setup, & workshop overview"
  },
  {
    "objectID": "slides/workshop_full_slides.html#meet-your-instructors",
    "href": "slides/workshop_full_slides.html#meet-your-instructors",
    "title": "DevOps for Data Scientists",
    "section": "Meet your instructors",
    "text": "Meet your instructors\n\nRikaAndrieDavidGaganMichael\n\n\n\n\nSolutions Engineer at Posit\nFormer Data Scientist and Data Engineer\n\n\n\n\n\nDirector of Posit Strategy, former Solutions Engineer\n\n\n\n\n\nDavid Aja is a Solutions Engineer at Posit. Before joining Posit, he worked as a data scientist in the public sector.\n\n\n\n\n\nGagandeep Singh is a former software engineer and data scientist who has worked in a variety of cross-technology teams before joining Posit as a Solutions Engineer.\n\n\n\n\n\nScientist by training turned IT / HPC / Scientific Computing"
  },
  {
    "objectID": "slides/workshop_full_slides.html#solutions-engineering-at-posit",
    "href": "slides/workshop_full_slides.html#solutions-engineering-at-posit",
    "title": "DevOps for Data Scientists",
    "section": "Solutions Engineering at Posit",
    "text": "Solutions Engineering at Posit\n\n\nPosit‚Äôs Solutions Engineering team aims to shrink the distance between the needs of Posit‚Äôs customers and our Pro and Open Source offerings, leading with curiosity and technical excellence.\nOur customer-facing work helps our customers deploy, install, configure, and use our Pro products."
  },
  {
    "objectID": "slides/workshop_full_slides.html#special-thanks-to-alex-gold",
    "href": "slides/workshop_full_slides.html#special-thanks-to-alex-gold",
    "title": "DevOps for Data Scientists",
    "section": "Special Thanks to Alex Gold!",
    "text": "Special Thanks to Alex Gold!\n\nAuthor of DevOps for Data Science\nPosit Solutions Engineering Team Director"
  },
  {
    "objectID": "slides/workshop_full_slides.html#introduce-yourself-to-your-neighbor",
    "href": "slides/workshop_full_slides.html#introduce-yourself-to-your-neighbor",
    "title": "DevOps for Data Scientists",
    "section": "Introduce yourself to your neighbor",
    "text": "Introduce yourself to your neighbor\n\nTake 5 minutes and introduce yourself to your neighbors."
  },
  {
    "objectID": "slides/workshop_full_slides.html#word-cloud",
    "href": "slides/workshop_full_slides.html#word-cloud",
    "title": "DevOps for Data Scientists",
    "section": "Word Cloud",
    "text": "Word Cloud"
  },
  {
    "objectID": "slides/workshop_full_slides.html#if-your-wifi-isnt-working-let-us-know-as-soon-as-possible",
    "href": "slides/workshop_full_slides.html#if-your-wifi-isnt-working-let-us-know-as-soon-as-possible",
    "title": "DevOps for Data Scientists",
    "section": "üíª If your WIFI isn‚Äôt working let us know as soon as possible!",
    "text": "üíª If your WIFI isn‚Äôt working let us know as soon as possible!\n\nName: Posit Conf 2023\nPassword: conf2023"
  },
  {
    "objectID": "slides/workshop_full_slides.html#pre-workshop-install",
    "href": "slides/workshop_full_slides.html#pre-workshop-install",
    "title": "DevOps for Data Scientists",
    "section": "Pre-workshop Install",
    "text": "Pre-workshop Install\n\nRequired:\nhttps://registry.hub.docker.com/signup\nhttps://github.com/signup\n\n\nOptional: (for local development)\nhttps://docs.docker.com/desktop/\nhttps://git-scm.com/book/en/v2/Getting-Started-Installing-Git\nhttps://quarto.org/docs/get-started/\nhttps://posit.co/downloads/"
  },
  {
    "objectID": "slides/workshop_full_slides.html#workshop-install",
    "href": "slides/workshop_full_slides.html#workshop-install",
    "title": "DevOps for Data Scientists",
    "section": "Workshop Install",
    "text": "Workshop Install\n\n\n\n\nSoftware\nLink\nCredentials\n\n\n\n\nPosit Workbench & Connect\nhttp://pos.it/class\nClassroom ID: devops_workshop\n\n\nDocker Classroom\nhttps://labs.play-with-docker.com/\nDocker Hub credentials"
  },
  {
    "objectID": "slides/workshop_full_slides.html#documentation-communication",
    "href": "slides/workshop_full_slides.html#documentation-communication",
    "title": "DevOps for Data Scientists",
    "section": "Documentation & Communication",
    "text": "Documentation & Communication\n\nAll documents including slides are available on our website: https://posit-conf-2023.github.io/DevOps/\nDiscord channel - devops-for-data-scientists\n\nDuring labs and exercises,please place a post-it note on the back of your laptop for:\nüü• - I need help\nüü© - I‚Äôm done"
  },
  {
    "objectID": "slides/workshop_full_slides.html#daily-schedule",
    "href": "slides/workshop_full_slides.html#daily-schedule",
    "title": "DevOps for Data Scientists",
    "section": "Daily Schedule",
    "text": "Daily Schedule\n\nüìÖ September 17 and 18, 2023\n‚è∞ 09:00am - 5:00pm\n\n\n\n\nTime\nTopic\n\n\n\n\n10:30 - 11:00am\nBreak\n\n\n12:30 - 1:30pm\nLunch\n\n\n3:00 - 3:30pm\nBreak"
  },
  {
    "objectID": "slides/workshop_full_slides.html#code-of-conduct",
    "href": "slides/workshop_full_slides.html#code-of-conduct",
    "title": "DevOps for Data Scientists",
    "section": "Code of Conduct",
    "text": "Code of Conduct\n\nEveryone who comes to learn and enjoy the experience should feel welcome at posit::conf. Posit is committed to providing a professional, friendly and safe environment for all participants at its events, regardless of gender, sexual orientation, disability, race, ethnicity, religion, national origin or other protected class.\nThis code of conduct outlines the expectations for all participants, including attendees, sponsors, speakers, vendors, media, exhibitors, and volunteers. Posit will actively enforce this code of conduct throughout posit::conf.\n\n\nhttps://posit.co/code-of-conduct/"
  },
  {
    "objectID": "slides/workshop_full_slides.html#workshop-goals",
    "href": "slides/workshop_full_slides.html#workshop-goals",
    "title": "DevOps for Data Scientists",
    "section": "Workshop Goals",
    "text": "Workshop Goals\n\nTo understand how DevOps can help you in your work as data scientists\nTo understand the main principles and tools of DevOps\nTo get hands-on experience putting code into production using common DevOps workflows\nTo leave the workshop with some ‚Äúassets‚Äù and skills you can use in your work"
  },
  {
    "objectID": "slides/workshop_full_slides.html#agenda-lab-overview",
    "href": "slides/workshop_full_slides.html#agenda-lab-overview",
    "title": "DevOps for Data Scientists",
    "section": "Agenda & Lab Overview",
    "text": "Agenda & Lab Overview\n\n\n\nSection/Time\nTopics\nLabs\n\n\n\n\nPart 1\nWorkshop overview\nLogistics & setup\nIntroductions\nInfrastructure & wifi setup\n\n\nPart 2: DevOps Principles & Tools\nIntroduction to DevOps\nVersion control & github\nCI/CD\nReproducing workflows and environments\nLab 1: Deploy your own Quarto website on Github Pages & Posit Connect using GitHub Actions |\n\n\nPart 3: Docker for Data Scientists\nHow and why data scientists use docker in production\nDocker: overview and architecture\nBuilding docker images and containers\nPorts & networking\nLab #2: Write your own Dockerfile to deploy Open Source Shiny Server on Docker playground and host an app on the server\n\n\nPart 4: Data Science in Production\nChoosing your deployment method\nAPIs and when to use them\nJust enough auth\nLogging & metrics & testing\nLab #3: Host and secure an API on Posit Connect\n\n\nPart 5: Discussion\nCourse feedback\nQuestions for the team\nWorkshop Survey"
  },
  {
    "objectID": "slides/workshop_full_slides.html#most-important-linux-commands",
    "href": "slides/workshop_full_slides.html#most-important-linux-commands",
    "title": "DevOps for Data Scientists",
    "section": "Most important Linux commands",
    "text": "Most important Linux commands\n\n\n\nCommand\nDescription\nExamples\n\n\n\n\ncd\nchange directory\nhttps://linuxize.com/post/linux-cd-command/\n\n\nls\nlist all files in current working directory;\nadd -lha flag for hidden files\nhttps://linuxize.com/post/how-to-list-files-in-linux-using-the-ls-command/\n\n\npwd\nprint working directory\nhttps://linuxize.com/post/current-working-directory/\n\n\ntouch\ncreate a file\nhttps://linuxize.com/post/linux-touch-command/\n\n\nmkdir\ncreate a directory\nhttps://linuxize.com/post/how-to-create-directories-in-linux-with-the-mkdir-command/\n\n\nvim\nopen a file in Vim text editor\nhttps://linuxize.com/post/how-to-save-file-in-vim-quit-editor/\n\n\ncurl\ncommand-line utility for transferring data from or to a server. Uses one of the supported protocols including HTTP, HTTPS, SCP , SFTP , and FTP\nhttps://linuxize.com/post/curl-command-examples/\n\n\necho\nprint argument to standard output\nhttps://linuxize.com/post/echo-command-in-linux-with-examples/\n\n\n$PATH\nenvironmental variable that includes colon-delimited list of directories where the shell searches for executable files\nhttps://linuxize.com/post/how-to-add-directory-to-path-in-linux/"
  },
  {
    "objectID": "slides/workshop_full_slides.html#what-we-wont-cover",
    "href": "slides/workshop_full_slides.html#what-we-wont-cover",
    "title": "DevOps for Data Scientists",
    "section": "What we won‚Äôt cover",
    "text": "What we won‚Äôt cover\n\n\n\nHow to become a DevOps engineer\nPython-based workflows\nIn-depth security and auth practices"
  },
  {
    "objectID": "slides/workshop_full_slides.html#part-2-devops-principles-tools",
    "href": "slides/workshop_full_slides.html#part-2-devops-principles-tools",
    "title": "DevOps for Data Scientists",
    "section": "Part 2: DevOps Principles & Tools",
    "text": "Part 2: DevOps Principles & Tools"
  },
  {
    "objectID": "slides/workshop_full_slides.html#section-goals",
    "href": "slides/workshop_full_slides.html#section-goals",
    "title": "DevOps for Data Scientists",
    "section": "Section Goals",
    "text": "Section Goals\n\n\nTo understand the main principles of DevOps and how they can improve data science workflows.\nTo get familiar with the DevOps toolkit.\nTo get familiar with how the renv package helps create reproducible environments for your R projects.\nTo get comfortable using the terminal for interacting with git and github.\nTo understand how to authenticate to github using SSH or HTTPS.\nTo practice creating a CI/CD workflow using yaml and Github Actions."
  },
  {
    "objectID": "slides/workshop_full_slides.html#section",
    "href": "slides/workshop_full_slides.html#section",
    "title": "DevOps for Data Scientists",
    "section": "",
    "text": "‚ÄúDevOps is a set of cultural norms, practices, and supporting tooling to help make the process of developing and deploying software smoother and lower risk.‚Äù\n\n\nDefinition credit:Alex Gold, https://www.do4ds.com\n\n\nLet‚Äôs get into it. We‚Äôre at a DevOps workshop - so what is it. This is a best effort attempt by our very own Alex Gold, but its still a squishy broad buzz word that‚Äôs defined differently across organization. Let‚Äôs discuss how the concept of devops came about."
  },
  {
    "objectID": "slides/workshop_full_slides.html#but-firsta-very-brief-history-lesson",
    "href": "slides/workshop_full_slides.html#but-firsta-very-brief-history-lesson",
    "title": "DevOps for Data Scientists",
    "section": "But first‚Ä¶a very brief history lesson",
    "text": "But first‚Ä¶a very brief history lesson\n\n\nReason for squishy definition is because it developed organically in the 2007-2009, across social media, twitter tag #devops was popular, online communities and became more organized through conferences, workshops, and grassroots attempts at implementation at diff companies. Then was introduced into larger companies like Intel, PayPal, Facebook.\nBecame more popular through work by Gene Kim and Patrick Debois and others - Pheonix Project is actually a novel about an IT project at a made-up company.\nSo what were all these people trying to do - what was the context? All trying to create something."
  },
  {
    "objectID": "slides/workshop_full_slides.html#so-you-want-to-create-an-app",
    "href": "slides/workshop_full_slides.html#so-you-want-to-create-an-app",
    "title": "DevOps for Data Scientists",
    "section": "So you want to create an app?",
    "text": "So you want to create an app?\n\n\nLet‚Äôs talk about what it looks like to create an application. On one side you had developers creating some sort of application - let‚Äôs say a stock trading application, so your end user are stock traders\nDevs would work on creating the code for this app, get requirements, test it and then package up all the source code in some way so that its executable and that it could be deployed somehow to those end users you‚Äôd configure the server, install any tools and frameworks needed to run the code, and then launch.\nNow your stock traders are using their application - maybe they‚Äôre giving feedback somehow or errors are being logged. Another team is monitoring the environment and seeing if servers are able to handle the load of all the users.\nBut eventually you‚Äôd need to fix bugs or release new features or maybe change things so that the app could handle more end users -\nyour dev team make updates, version those changes, test them, re-launch the app and you do this over and over again in some sort or release process ‚Äì so you have this continuous cycle of releasing your app, finding something to fix or add, testing that change, integrating that code, and then releasing the app..\nMaking this continuous process as fast, efficient, accurate, and low risk is the goal of devops.\nsounds pretty efficient right -"
  },
  {
    "objectID": "slides/workshop_full_slides.html#problems-devops-tries-to-solve",
    "href": "slides/workshop_full_slides.html#problems-devops-tries-to-solve",
    "title": "DevOps for Data Scientists",
    "section": "Problems DevOps tries to solve",
    "text": "Problems DevOps tries to solve\n\n\n\nSo this is the idealized example - but what was it like in practice 20 years ago. You want to make the release process as fast as possible but you also want it to be tested and free of bugs. And this introduces a natural conflict between speed and stability\nYou have the same goals of speed and accuracy - but this process of continuous integration, of testing, of automating processes so that your users can get their hands on the app - didnt exist.\nOne problem is that the entire process is siloed between the creation of the code and the deployment of your app. So your developers finish their code and then throw it over the fence to ops. Maybe they then throw it to security or to QA. But there was no formal alliance in place or processes on how the teams work together. Because each team is seemingly working on one part - the code vs.¬†the deploy - there‚Äôs also technical knowledge siloes - each team only knows their bit but not anything else.\nSo imagine this stock trading app - your developers are creating cool new features so that you can easily buy and sell stock, maybe even see data on whats happening in the market. But they‚Äôre not necessarily thinking about how the app is secured or if its compliant with federal regulations or if user data is protected. Obvi this is a worst case scenario - but you get the picture.\nSo as the app is built it keeps getting thrown back and forth between the two teams and leads to one of the major problems that devops is trying to fix - really slow and manual bureaucratic release process.\nOn the technical side, a lot of the actual work is done manually. So you can imagine one team running tests manually in one environment - maybe the environment itself is created manually, or someone manually sizing up or fixing a server. This takes time, you need to get approval from people, and its not easily reproduced or really documented anywhere. Also, there is a lot of room for error and if things break its not easy to roll back bugs.\nProblem becomes how do you automate this release process, make it streamlined, less error prone, improve how all these teams integrate and operate, and at the end of the day make the process of getting the app into the hands of your users much faster."
  },
  {
    "objectID": "slides/workshop_full_slides.html#common-devops-principles",
    "href": "slides/workshop_full_slides.html#common-devops-principles",
    "title": "DevOps for Data Scientists",
    "section": "Common DevOps Principles",
    "text": "Common DevOps Principles\n\n\nü§≤ Collaboration\n\nüöö Continuous Integration & Delivery\n\nü§ñ Automation\n\nüìî Reproducibility\n\nüë®‚Äçüé§ Culture change\n\nAs the field of devops developed, there are different implementations/diff technologies of what devops means - these happen differently at different companies But there have been some common philosophies and best practices that have come out that I‚Äôve seen consistently across the entire ecosystem.\nFirst - everyone that‚Äôs involved in the creation of this application should be collaborating and working together. This is my wink wink moment for you all of why devops is so important for data scientists\nThen we have this concept of Continuous Integration & Delivery - we‚Äôll take about this in greater length in a few minutes - but this is the process of 1. the coding of the app 2. the versioning of that code is some kind of repository 3. testing those changes 4. integrating them into the app if those tests are passed 5. And then that continuous process of releasing those changes into production\nSo for the CI/CD process to work and specifically to work quickly - we will need some level of automation as we move through that cyclical process. You don‚Äôt want to have to pause things to get approval or manually change things when tests pass - you want it to get to the user as quickly as possible after everything has been tested.\nThe next piece is reproducibility. You have different teams working on different pieces of the puzzle, but at the end of the day the end result is the same. Everyone is working on that same app - in this case our stock platform. So as jobs go from one team to another team and as your app goes from development to testing to finally your users, you want to make sure that there‚Äôs a common ecoystem that everyone is working in. When you‚Äôre testing your app you want to make sure that your environment is as similar to what is going to be in real-life or what is called ‚Äúproduction‚Äù. You also want to make sure that your code itself is reproducible and that it works for users regardless of their location or their computer or their browser maybe.\nThe last piece that I find really interesting is the concept of culture change. So what needs to change in your organization so that you‚Äôre able to function while still holding this tension or conflict between speed and stability. How do you make sure that teams that have different incentives - for example security team vs.¬†a data team - are able to work together.\nSo let me pause - that‚Äôs a lot - everyone got a bit of a history lesson but as far as i‚Äôm aware no one here is a software developer or an IT engineer. So that brings me to this very important question: But"
  },
  {
    "objectID": "slides/workshop_full_slides.html#why-should-we-as-data-scientists-care-about-this",
    "href": "slides/workshop_full_slides.html#why-should-we-as-data-scientists-care-about-this",
    "title": "DevOps for Data Scientists",
    "section": "Why should we as data scientists care about this?",
    "text": "Why should we as data scientists care about this?\n\nlets have a little thought experiment"
  },
  {
    "objectID": "slides/workshop_full_slides.html#has-this-ever-happened-to-you",
    "href": "slides/workshop_full_slides.html#has-this-ever-happened-to-you",
    "title": "DevOps for Data Scientists",
    "section": "Has this ever happened to you?",
    "text": "Has this ever happened to you?\n\n\nYou come back to code from a year ago and now it doesnt run!\n\nReproducibility\n\n\n\n\nYou need to hand off your model to the Engineering team but they only code in Java\n\nContinuous Integration & Delivery & Collaboration\n\n\n\n\nYour boss asks you to share that Shiny app with a client but the ops team is too busy working on their roadmap to help you deploy it somewhere.\n\nCulture Change"
  },
  {
    "objectID": "slides/workshop_full_slides.html#we-should-care-because",
    "href": "slides/workshop_full_slides.html#we-should-care-because",
    "title": "DevOps for Data Scientists",
    "section": "We should care because ‚Ä¶",
    "text": "We should care because ‚Ä¶\n\n\n\nData scientists are developers!\nData science careers have moved from academic sphere to tech and software but education hasnt always followed\nAutomation, collaboration, testing can dramatically improve data work and improve reproducibility\nThe many hats of a data scientist\nImprove collaboration & communication with other teams\n\n\n\nspent a lot of time talking about teams working together to create a shared product. Data scientists are a part of that team. DS is a relatively new field and creating data-intensive apps is a huge part of tech world today.\nDS career paths started in academia - not a lot of best practices are taught"
  },
  {
    "objectID": "slides/workshop_full_slides.html#responsibility-of-the-analyst",
    "href": "slides/workshop_full_slides.html#responsibility-of-the-analyst",
    "title": "DevOps for Data Scientists",
    "section": "Responsibility of the analyst",
    "text": "Responsibility of the analyst\n\n\nI mentioned before that a lot of DS comes from academia and research. So the concept of reproducing your results - especially if those results play a role in how medicines or medical devices are developed - is incredibly important.\n2005 essay by John Ioannidis in scientific journal on what is known as the replicability crisis in scientific publishing, arguing that majority medical research studies cannot be replicated.\nSo as data scientists and we are curators of the data and are responsible for its legitimacy in many ways - obv cant always control it of course.\nThis second paper started looking a method of reproducing results - namely Jupyter notebooks - and found a lot of ways that reproducibility was improved but also a lot of places where things could have been done better and some best practices for how to do that - in what we call literate programming.\n27,271 Notebooks:\n\n11,454 could not declare dependencies\n5,429 could not successfully install declared dependencies\n9,185 returned an error when ran\n324 returned a different result than originally reported\n\nLet‚Äôs get back to our principles:"
  },
  {
    "objectID": "slides/workshop_full_slides.html#the-devops-process-flow",
    "href": "slides/workshop_full_slides.html#the-devops-process-flow",
    "title": "DevOps for Data Scientists",
    "section": "The Devops Process Flow",
    "text": "The Devops Process Flow"
  },
  {
    "objectID": "slides/workshop_full_slides.html#proliferation-of-tools",
    "href": "slides/workshop_full_slides.html#proliferation-of-tools",
    "title": "DevOps for Data Scientists",
    "section": "Proliferation of tools",
    "text": "Proliferation of tools\n\n\nproliferation of tools, source code management, repo management, build tools, data management, deployment, container services, config management, monitoring, cloud services"
  },
  {
    "objectID": "slides/workshop_full_slides.html#an-opinionated-take-on-tools",
    "href": "slides/workshop_full_slides.html#an-opinionated-take-on-tools",
    "title": "DevOps for Data Scientists",
    "section": "An opinionated take on tools",
    "text": "An opinionated take on tools\n\n\nCI/CD\nEnvironment Management\nVersion Control & Git workflows\nPackage Management\nAutomation & Build tools\nContinuous Monitoring and Logging\n\n\n-these are the ones that we‚Äôre going to be covering - lot of tools, but this is what they should help you to do -definitely not going to be a complete overview of the entire ecosystem - there‚Äôs also a lot of overlap in the definitions -\n-but in my opinion the most important tool types to get you started quickly."
  },
  {
    "objectID": "slides/workshop_full_slides.html#the-cicd-pipeline",
    "href": "slides/workshop_full_slides.html#the-cicd-pipeline",
    "title": "DevOps for Data Scientists",
    "section": "The CI/CD Pipeline",
    "text": "The CI/CD Pipeline\n\n\n\nAs we discussed before CI/CD is an iterative cycle of small steps to quickly build, test, and deploy your code - this is a critical component of devops.\nSo CI/CD is often said in the same breath - as it makes up a continuous pipeline - but its actually pretty discrete parts so I want to make sure we understand how they differ.\n\n-Continuous integration (CI) - this is where you or anyone who writes code, automatically builds, tests, and commits code changes into a shared repository; This is usually triggered by an automatic process where the code is built, tested and then either passes or fails. This step is focused on improving the actual build or application as quickly as possible.\nDifferent types of tests - from unit tests to integration tests to regression tests.\n-Continuous delivery (CD) and deployment are less focused on the build but on the actual installation and distribution of your build. This includes an automated process to deploy across different environments - from development to testing and finally to production. Delivery is the process for the final release - that ‚Äúpush of a button step‚Äù to get to prod.\nwe‚Äôll talk about some examples of popular CI/CD tools in a bit - Github Actions, Jenkins, CircleCI, Gitlab CI"
  },
  {
    "objectID": "slides/workshop_full_slides.html#environment-management",
    "href": "slides/workshop_full_slides.html#environment-management",
    "title": "DevOps for Data Scientists",
    "section": "Environment Management",
    "text": "Environment Management\n\n\n\n\n\n\n\n\n\nDevelopment\nTesting\nProduction\n\n\n\n\n\nExploratory\nOften local machine\nAccess to R/Python Packages\n\n\nas similar to prod as possible\nunit & integration testing\ndata validation\n‚Äúsandbox‚Äù with data that‚Äôs as close to real as possible\n\n\nautomatic CD\nisolated from dev & test\ncreated with code\n\n\n\n\n\nunit test - testing individual units of code to make sure it does what you want it to do\nintegration test - make sure putting pieces together doesnt introduce any errors\nwant to be able to reproduce the environments as closely as possible"
  },
  {
    "objectID": "slides/workshop_full_slides.html#version-control-workflows",
    "href": "slides/workshop_full_slides.html#version-control-workflows",
    "title": "DevOps for Data Scientists",
    "section": "Version Control & Workflows",
    "text": "Version Control & Workflows\n\n\nVersion control is a main tool for Continuous Integration -\nlots of variants - git, github., svn, gitlab, etc\ndistributed version control - everyone has their own repo\ncan track changes and roll them back\ncan fix merge conflicts\nan an iterative process to build, test, collaborate on your code to above environments. Very commonly, individuals work on separate branches that are then tested and reviewed by colleagues before they are merged into a main branch.\nIn addition to the action of promoting your code - its also important to have processes in place for how the code integration process occurs - includes humans coming together and making some decisions -\ne.g code reviews, process for your team, how to name things, pull requests, merging, feature branching, automatic tests"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---developing-locally-using-quarto",
    "href": "slides/workshop_full_slides.html#lab-activity---developing-locally-using-quarto",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - developing locally using Quarto",
    "text": "Lab Activity - developing locally using Quarto\nüü• - I need help\nüü© - I‚Äôm done\n\nLogin to pos.it/class with code devops_workshop\nLab 1 Part 1\n\nShow how to login to the workshop environment and where the lab is\n5 minutes"
  },
  {
    "objectID": "slides/workshop_full_slides.html#is-version-control-secure",
    "href": "slides/workshop_full_slides.html#is-version-control-secure",
    "title": "DevOps for Data Scientists",
    "section": "Is version control secure?",
    "text": "Is version control secure?"
  },
  {
    "objectID": "slides/workshop_full_slides.html#a-short-auth-teaser",
    "href": "slides/workshop_full_slides.html#a-short-auth-teaser",
    "title": "DevOps for Data Scientists",
    "section": "A short auth teaser",
    "text": "A short auth teaser\n\nWe can use a variety of data sharing ‚Äútransfer protocols‚Äù\nProtocols specify what kind of traffic is moving between 2 machines\nUse different security mechanisms\nPorts on the host and destination specifies where to direct the traffic\n\n\nHTTPS\n\n\nAdd diagram - http and https two computers with ports\nThere are lots of diff transfer protocols; what you use depends on what kind data is being transferred and your security requirements. So for example, maybe you‚Äôre sending plain text or email or a webpage.\nProtocols as well as processes or application run on specific ports. A port is a virtual point where network connections start and end. Allow computers to easily differentiate between different kinds of traffic. So for example, Shiny Server runs on port 3838, Workbench runs on 8787\nwhether its email, text, files, etc Ports are software-based and managed by a computer‚Äôs operating system. Ports emails go to a different port than webpages, for instance, even though both reach a computer over the same Internet connection."
  },
  {
    "objectID": "slides/workshop_full_slides.html#git-protocol-options",
    "href": "slides/workshop_full_slides.html#git-protocol-options",
    "title": "DevOps for Data Scientists",
    "section": "Git protocol options",
    "text": "Git protocol options\n\n\nhttp - 80\nhttps - 443\nssh - 22\nwe‚Äôll be using https - because ‚Ä¶ from jenny bryans happy git with R\nI find HTTPS easier to get working quickly and strongly recommend it when you first start working with Git/GitHub.\nHTTPS is what GitHub recommends, presumably for exactly the same reasons.\nThe ‚Äúease of use‚Äù argument in favor of HTTPS is especially true for Windows users.\nAnother advantage of HTTPS is that the PAT we‚Äôll set up for that can also be used with GitHub‚Äôs REST API.\nhypertext transfer protocol When a web user wants to load or interact with a web page, their web browser sends an HTTP request to the origin server that hosts the website‚Äôs files. These requests are essentially lines of text that are sent via the internet.\nhttps : http encrypted with SSL/TLS - digital certificates that establish an encrypted connected\nSSH: secure shell, public-key cryptography to authenticate the client, used for remote logins, command line execution\nHTTPS is simpler. For most services besides Github, you just have to enter in your username and password, and you‚Äôll be able to push and pull code.\nYou don‚Äôt have to juggle multiple SSH keys around to use multiple devices.\nPort 443, which HTTPS uses, is open in basically any firewall that can access the internet. That isn‚Äôt always the case for SSH.\nThe primary downside for most people is that you must enter your Git password/token every time you push. While it gets added to a cache, it‚Äôs not configured to cache permanently (though this can be changed). With SSH keys, it just uses the key file on disk every time.\nWhere SSH takes the lead is with the authentication factor‚Äîthe key. The length of it alone makes it harder to accidentally leak, and due to it being unwieldy and unique, it‚Äôs generally more secure."
  },
  {
    "objectID": "slides/workshop_full_slides.html#reproducing-your-environment",
    "href": "slides/workshop_full_slides.html#reproducing-your-environment",
    "title": "DevOps for Data Scientists",
    "section": "Reproducing your environment",
    "text": "Reproducing your environment\n\nWhat are the layers that need to be reproduced across your dev, test, and prod environments?\n\nWhat‚Äôs your most difficult reproducibility challenge?"
  },
  {
    "objectID": "slides/workshop_full_slides.html#layers-of-reproducibility",
    "href": "slides/workshop_full_slides.html#layers-of-reproducibility",
    "title": "DevOps for Data Scientists",
    "section": "Layers of reproducibility",
    "text": "Layers of reproducibility\n\n\n\ncode - scripts, configs, applications\nPackages\nSystem - r and python depend on underlying system software - for example, spatial analysis packages, or anything that requires Java - rJava\nOS\nHardware - processors Intel chip, silicon chip"
  },
  {
    "objectID": "slides/workshop_full_slides.html#packages-vs.-libraries-vs.-repositories",
    "href": "slides/workshop_full_slides.html#packages-vs.-libraries-vs.-repositories",
    "title": "DevOps for Data Scientists",
    "section": "Packages vs.¬†Libraries vs.¬†Repositories",
    "text": "Packages vs.¬†Libraries vs.¬†Repositories\n\nPackage - contains code, functions, data, and documentation.\n\nLibrary - is a directory where packages are installed.\n\nRepository - a collection of packages. CRAN is a public external repository that is a network of servers that distribute R along with R packages.\n\npackages - Can be be distributed as SOURCE (a directory with all package components), BINARIES (contains files in OS-specific format) or as a BUNDLE (compressed file containing package components, similar to source).\nlibrary - You can have user-level or project-level libraries. Run .libPaths() to see yours. To use a package in has to be installed in a library with install.packages() and then loaded into memory with library(x) .\nrepo - others include pypi, bioconducter, private repos"
  },
  {
    "objectID": "slides/workshop_full_slides.html#renv-workflow",
    "href": "slides/workshop_full_slides.html#renv-workflow",
    "title": "DevOps for Data Scientists",
    "section": "renv workflow",
    "text": "renv workflow\n\n\n\nUse a version control system e.g.git with GitHub\nOne user (should explicitly initialize renv in the project, This will create the initial renv lockfile, and also write the renv auto-loaders to the project‚Äôs .Rprofile and renv/activate.R. These will ensure the right version of renv is downloaded and installed for your collaborators when they start in this project.\npush your code alongside the generated lockfile renv.lock. Be sure to also share the generated auto-loaders in .Rprofile and renv/activate.R.\nAfter this has completed, they can then use renv::restore() to restore the project library locally on their machine."
  },
  {
    "objectID": "slides/workshop_full_slides.html#example---renv",
    "href": "slides/workshop_full_slides.html#example---renv",
    "title": "DevOps for Data Scientists",
    "section": "Example - renv",
    "text": "Example - renv\n\n# install.packages(\"renv\")\nrenv::init()\nrenv::snapshot()\nlapply(.libPaths(), list.files)\n\n\nüîç Live code"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---renv-workflow-github-authentication",
    "href": "slides/workshop_full_slides.html#lab-activity---renv-workflow-github-authentication",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - renv workflow & github authentication",
    "text": "Lab Activity - renv workflow & github authentication\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 1: Part 2 & 3\n\nYou‚Äôll be doing renv in your workbench environment both in your console for r code and in the terminal for git commands you‚Äôll also need to log in to your github account where you‚Äôll be creating an upstream repository to link with your local code.\nwe‚Äôll be using https auth but there‚Äôs also information if you later want to test out ssh\n10-15 minutes"
  },
  {
    "objectID": "slides/workshop_full_slides.html#build-tools",
    "href": "slides/workshop_full_slides.html#build-tools",
    "title": "DevOps for Data Scientists",
    "section": "Build tools",
    "text": "Build tools\n\nVirtualization/Containerization\nInfrastructure as code\nAutomation with Github Actions/Jenkins/CircleCI\nCloud devops tools (Azure Devops, Google Cloud Build)\n\n\n\nautomate common build and deployment tasks\nfor example, generating artifacts, deploying to diff environs, provisioning or deprovisioning environments -\nsome overlap here between these tools and CI/CD\n\nVirtual/Container -prod-identical - help you build isolated environments and tested and debug code; docker, kubernetes, other container services\nIaC - quickly re-provision your entire environment with code (chef, puppet, dockr, terraform, ansible) instead of fixing it, just tear it down and restart it - configuration management\ntools that automate the process of building and deploying your code once it goes from your dev env to test and prod\ncloud devops - run your entire devops stack in a cloud managed cloud managed platform\nYou also have support tools - that are used across a lot of these tools - I want to discuss one of them here - YAML\n\n\nIllustration credit:"
  },
  {
    "objectID": "slides/workshop_full_slides.html#power-of-yaml",
    "href": "slides/workshop_full_slides.html#power-of-yaml",
    "title": "DevOps for Data Scientists",
    "section": "Power of YAML",
    "text": "Power of YAML\n\nYAML Ain‚Äôt Markup Language\ncommunication of data between people and computers\nhuman friendly\nconfigures files across many execution environments"
  },
  {
    "objectID": "slides/workshop_full_slides.html#yaml",
    "href": "slides/workshop_full_slides.html#yaml",
    "title": "DevOps for Data Scientists",
    "section": "YAML",
    "text": "YAML\nEmpRecord:  \n  emp01:\n    name: Michael\n    job: Manager\n    skills: \n      - Improv\n      - Public speaking\n      - People management\n  emp02:\n    name: Dwight\n    job: Assistant to the Manager\n    skills: \n      - Martial Arts\n      - Beets\n      - Sales\n\nwhitespace indentation denotes structure & hierarchy\nColons separate keys and their values\nDashes are used to denote a list"
  },
  {
    "objectID": "slides/workshop_full_slides.html#example---yaml-in-action",
    "href": "slides/workshop_full_slides.html#example---yaml-in-action",
    "title": "DevOps for Data Scientists",
    "section": "Example - YAML in action",
    "text": "Example - YAML in action\n\nDocker compose yaml file to spin up all 3 Posit Pro products\n\n\nüîç Live code"
  },
  {
    "objectID": "slides/workshop_full_slides.html#exercise---inspect-your-yaml",
    "href": "slides/workshop_full_slides.html#exercise---inspect-your-yaml",
    "title": "DevOps for Data Scientists",
    "section": "Exercise - Inspect your YAML",
    "text": "Exercise - Inspect your YAML\nüü• - I need help\nüü© - I‚Äôm done\n\nInspect your _quarto.yml file and identify what each part of it does using the quarto site.\n\n2-5 minutes - render your site to help"
  },
  {
    "objectID": "slides/workshop_full_slides.html#continuous-integration-example",
    "href": "slides/workshop_full_slides.html#continuous-integration-example",
    "title": "DevOps for Data Scientists",
    "section": "Continuous Integration Example",
    "text": "Continuous Integration Example\n\n\ntoday - we‚Äôll be discussing GHA Actions - which is a common CI tool that is handy because it uses the same platform as your versioned repositories - inthis case github\nuses yaml as a configuration tool"
  },
  {
    "objectID": "slides/workshop_full_slides.html#other-gha-categories",
    "href": "slides/workshop_full_slides.html#other-gha-categories",
    "title": "DevOps for Data Scientists",
    "section": "Other GHA Categories",
    "text": "Other GHA Categories\n\nDeployment\nSecurity\nContinuous Integration\nAutomation\nPages\nMake your own!\n\nExamples"
  },
  {
    "objectID": "slides/workshop_full_slides.html#quarto-publish-flows",
    "href": "slides/workshop_full_slides.html#quarto-publish-flows",
    "title": "DevOps for Data Scientists",
    "section": "Quarto Publish Flows",
    "text": "Quarto Publish Flows\nManually\n\nContinuous Integration\n\n\ncan publish to quarto pub, GHP, connect, netlify, confluence, other destinations, more work to set up"
  },
  {
    "objectID": "slides/workshop_full_slides.html#gha-structure",
    "href": "slides/workshop_full_slides.html#gha-structure",
    "title": "DevOps for Data Scientists",
    "section": "GHA Structure",
    "text": "GHA Structure\n\nOpen source ecosystem of available actions\n\nGithub Official Actions\nRLib Actions\n\n\nWorkflows can include tests, markdown renders, shell scripts, cron jobs, or deployments. They can be as simple or as complicated as you need. Open-source community provides a ton of examples of actions."
  },
  {
    "objectID": "slides/workshop_full_slides.html#actions-syntax",
    "href": "slides/workshop_full_slides.html#actions-syntax",
    "title": "DevOps for Data Scientists",
    "section": "Actions Syntax",
    "text": "Actions Syntax\nCode"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---cicd-with-github-actions",
    "href": "slides/workshop_full_slides.html#lab-activity---cicd-with-github-actions",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - CICD with Github Actions",
    "text": "Lab Activity - CICD with Github Actions\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 1: Part 4\n\nwe‚Äôll be doing a few things - create a gh branch - publish to github - create a gha with a yaml file - create an api key for connect - update yaml file - push everything"
  },
  {
    "objectID": "slides/workshop_full_slides.html#logging",
    "href": "slides/workshop_full_slides.html#logging",
    "title": "DevOps for Data Scientists",
    "section": "Logging",
    "text": "Logging\n\n\n\n\n\n\napp.py\n\nimport logging\n\n# Configure the log object\nlogging.basicConfig(\n    format='%(asctime)s - %(message)s',\n    level=logging.INFO\n)\n\n# Log app start\nlogging.info(\"App Started\")\n\n\n\n\n\n\napp.R\n\n# Configure the log object\nlog &lt;- log4r::logger()\n\n# Log app start\nlog4r::info(log, \"App Started\")\n\n\n\n\n\ncreate a log session for your r or python code, write a log statement, gets added into an entry\nwhat to log\ntypes of logs"
  },
  {
    "objectID": "slides/workshop_full_slides.html#monitoring",
    "href": "slides/workshop_full_slides.html#monitoring",
    "title": "DevOps for Data Scientists",
    "section": "Monitoring",
    "text": "Monitoring\n\nPrometheus\n\nOS monitoring toolkit\nofficial Prometheus client in Python\n{openmetrics} package to register metrics from a Plumber API or Shiny app.\n\n\nGrafana\n\nGUI for Prometheus\nGet Started with Grafana and Prometheus"
  },
  {
    "objectID": "slides/workshop_full_slides.html#part-3-docker-for-data-scientists",
    "href": "slides/workshop_full_slides.html#part-3-docker-for-data-scientists",
    "title": "DevOps for Data Scientists",
    "section": "Part 3: Docker for Data Scientists",
    "text": "Part 3: Docker for Data Scientists"
  },
  {
    "objectID": "slides/workshop_full_slides.html#section-goals-1",
    "href": "slides/workshop_full_slides.html#section-goals-1",
    "title": "DevOps for Data Scientists",
    "section": "Section Goals",
    "text": "Section Goals\n\nto understand different docker workflows\nto learn common docker syntax for building and running containers\nto get hands on experience building and running containers\nto explore the linux file system"
  },
  {
    "objectID": "slides/workshop_full_slides.html#why-use-a-container-at-all",
    "href": "slides/workshop_full_slides.html#why-use-a-container-at-all",
    "title": "DevOps for Data Scientists",
    "section": "Why use a container at all?",
    "text": "Why use a container at all?\n\nallows you to package up everything you need to reproduce an environment/application\nlightweight system without much overhead\nshare containers with colleagues without requiring them to have to set up their own local machines\nquick testing and debugging\nallows you to easily version snapshots of your work\nscaling up"
  },
  {
    "objectID": "slides/workshop_full_slides.html#our-docker-environment",
    "href": "slides/workshop_full_slides.html#our-docker-environment",
    "title": "DevOps for Data Scientists",
    "section": "Our Docker Environment",
    "text": "Our Docker Environment"
  },
  {
    "objectID": "slides/workshop_full_slides.html#workflow",
    "href": "slides/workshop_full_slides.html#workflow",
    "title": "DevOps for Data Scientists",
    "section": "Workflow",
    "text": "Workflow\n\n\ndockerfile - is a script of instructions for how to build an image\nimage - everything you need to run an application - all the layers that build the environment, dependencies, libraries, files\ncontainer - isolated instance of a running image. you can create, stop, start, restart, containers. When a container is removed/deleted any changes to its state that arent stored in some kind of persistent storage disappear. Called ephemeral container - Think of a container as a snapshot in time of a particular application.\nMissing piece - repository - for images, like dockerhub, container registry cloud services, private registries\nBuild - Run - Push"
  },
  {
    "objectID": "slides/workshop_full_slides.html#architecture",
    "href": "slides/workshop_full_slides.html#architecture",
    "title": "DevOps for Data Scientists",
    "section": "Architecture",
    "text": "Architecture\n\n\nOn the left we have the client - think of this as your computer or perhaps a computer somewhere in the cloud\non the right you have the server - this the docker host which runs something called the Docker daemon - you‚Äôll be communicating with the Docker daemon via a command line interface.\nThe host can be remote or it can be on your computer with the client - so for example, you can download something called Docker Engine. In most cases the host is running on a Linux OS. (in different laptop OS, Docker Desktop uses a lightweight VM under the hood)\nDocker engine is a container runtime that runs on different OS‚Äôs. Sets up the isolated environment for your container.\nwhat the heck is the Docker daemon - its a service that does all the heavy lifting of building and running your containers\nregistry - upload and download images\nDocker Desktop includes the Docker daemon (dockerd), the Docker client (docker), Docker Compose - another client, Docker Content Trust, Kubernetes, and Credential Helper."
  },
  {
    "objectID": "slides/workshop_full_slides.html#images",
    "href": "slides/workshop_full_slides.html#images",
    "title": "DevOps for Data Scientists",
    "section": "Images",
    "text": "Images\n\n\nmade up of individual layers so its really quick to build isolating applications into their own image"
  },
  {
    "objectID": "slides/workshop_full_slides.html#dockerhub-registry",
    "href": "slides/workshop_full_slides.html#dockerhub-registry",
    "title": "DevOps for Data Scientists",
    "section": "Dockerhub Registry",
    "text": "Dockerhub Registry\nhttps://hub.docker.com/search?q="
  },
  {
    "objectID": "slides/workshop_full_slides.html#containers",
    "href": "slides/workshop_full_slides.html#containers",
    "title": "DevOps for Data Scientists",
    "section": "Containers",
    "text": "Containers\n\n\nrunnable instance of an image.\nFrom the bottom up Containers run on some sort of infrastructure - this could be your laptop or a server somewhere in the cloud\nhost OS - shared OS between container instances, dont need a guest os for each one, makes them really light and able to spinup in milliseconds\ncontainers are very lightweight which makes it really easy and quick to spin them up\nInstead, the Docker daemon communicates directly with the host operating system and knows how to ration out resources for the running Docker containers."
  },
  {
    "objectID": "slides/workshop_full_slides.html#how-can-data-scientists-use-docker",
    "href": "slides/workshop_full_slides.html#how-can-data-scientists-use-docker",
    "title": "DevOps for Data Scientists",
    "section": "How can data scientists use docker?",
    "text": "How can data scientists use docker?"
  },
  {
    "objectID": "slides/workshop_full_slides.html#example---testing-versioning",
    "href": "slides/workshop_full_slides.html#example---testing-versioning",
    "title": "DevOps for Data Scientists",
    "section": "Example - testing & versioning",
    "text": "Example - testing & versioning\n\n\ndocker pull postgres:12\ndocker pull postgres:latest\ndocker image ls -a\ndocker run -d -e POSTGRES_PASSWORD=mysecretpassword --name postgres_early imageID\ndocker run -d -e POSTGRES_PASSWORD=mysecretpassword --name postgres_new imageID\ndocker container ls -a\ndocker stop\ndocker restart\n\nüîç Live code\n\n\ncan pull but also if you just run - will put it from hub if doenst find locally Let‚Äôs see an example of what this looks like in practice. Lets say we need to use a postgres sql database for some testing - but we want to test using an older and a newer version of postgres.\ndocker image ls - lets list all the images that we have\ndocker pull postgres:12 - see how its pulling and extracting all these layers - but what if we want a newer version or what if we need to run both versions on our machine\ndocker pull postgres:latest - notice how some of these layers already exist so it takes a lot less time"
  },
  {
    "objectID": "slides/workshop_full_slides.html#example---reproduce-environments",
    "href": "slides/workshop_full_slides.html#example---reproduce-environments",
    "title": "DevOps for Data Scientists",
    "section": "Example - reproduce environments",
    "text": "Example - reproduce environments\n\nhttps://hub.docker.com/u/rocker\n\n# pull the image\ndocker pull rocker/r-base\n\n# run container\ndocker run --rm -it rocker/r-base"
  },
  {
    "objectID": "slides/workshop_full_slides.html#example---isolate-applications",
    "href": "slides/workshop_full_slides.html#example---isolate-applications",
    "title": "DevOps for Data Scientists",
    "section": "Example - isolate applications",
    "text": "Example - isolate applications\n\nhttps://hub.docker.com/u/rstudio\n\nDO NOT RUN!\ndocker run -it --privileged \\\n    -p 3939:3939 \\\n    -e RSC_LICENSE=$RSC_LICENSE \\\n    rstudio/rstudio-connect:ubuntu2204"
  },
  {
    "objectID": "slides/workshop_full_slides.html#example---docker-as-deployment-strategy",
    "href": "slides/workshop_full_slides.html#example---docker-as-deployment-strategy",
    "title": "DevOps for Data Scientists",
    "section": "Example - Docker as deployment strategy",
    "text": "Example - Docker as deployment strategy\n\nhttps://github.com/mickwar/ml-deploy"
  },
  {
    "objectID": "slides/workshop_full_slides.html#example---docker-in-ci-pipeline",
    "href": "slides/workshop_full_slides.html#example---docker-in-ci-pipeline",
    "title": "DevOps for Data Scientists",
    "section": "Example - Docker in CI pipeline",
    "text": "Example - Docker in CI pipeline\njobs:\n  build:\n    runs-on: ubuntu-latest\n    steps:\n      -\n        name: Checkout\n        uses: actions/checkout@v3\n      -\n        name: Login to Docker Hub\n        uses: docker/login-action@v2\n        with:\n          username: ${{ secrets.DOCKERHUB_USERNAME }}\n          password: ${{ secrets.DOCKERHUB_TOKEN }}\n      -\n        name: Set up Docker Buildx\n        uses: docker/setup-buildx-action@v2\n      -\n        name: Build and push\n        uses: docker/build-push-action@v4\n        with:\n          context: .\n          file: ./Dockerfile\n          push: true\n          tags: ${{ secrets.DOCKERHUB_USERNAME }}/latest\n\n\nData scientists benefit from Docker‚Äôs consistency and reproducibility.\nCreate isolated environments for different experiments.\nShare work with colleagues without environment setup issues.\nConsistency: Containers ensure that applications run the same way across different environments.\nIsolation: Containers isolate applications and their dependencies, preventing conflicts.\nPortability: Containers can run on any system that supports Docker, reducing ‚Äúit works on my machine‚Äù issues."
  },
  {
    "objectID": "slides/workshop_full_slides.html#modes-for-running-containers",
    "href": "slides/workshop_full_slides.html#modes-for-running-containers",
    "title": "DevOps for Data Scientists",
    "section": "Modes for running containers",
    "text": "Modes for running containers\n\n\n\n\n\n\n\n\nMode\nRun command\nUse case\n\n\n\n\nDetached\ndocker run -d\nThis runs the container in the background so the container keeps running until the application process exits, or you stop the container. Detached mode is often used for production purposes.\n\n\nInteractive + terminal\ndocker run -it\nThis runs the container in the foreground so you are unable to access the command prompt. Interactive mode is often used for development and testing.\n\n\nRemove everything once the container is done with its task\ndocker run --rm\nThis mode is used on foreground containers that perform short-term tasks such as tests or database backups. Once it is removed anything you may have downloaded or created in the container is also destroyed."
  },
  {
    "objectID": "slides/workshop_full_slides.html#run-command",
    "href": "slides/workshop_full_slides.html#run-command",
    "title": "DevOps for Data Scientists",
    "section": "Run Command",
    "text": "Run Command"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---running-containers",
    "href": "slides/workshop_full_slides.html#lab-activity---running-containers",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - Running containers",
    "text": "Lab Activity - Running containers\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 2: Part 1"
  },
  {
    "objectID": "slides/workshop_full_slides.html#running-containers",
    "href": "slides/workshop_full_slides.html#running-containers",
    "title": "DevOps for Data Scientists",
    "section": "Running containers",
    "text": "Running containers\n\ndocker pull ubuntu\ndocker image ls -a\n\ndocker run -it ubuntu bash\nls\nwhoami\nhostname\n# exit the container with Ctrl+D or exit\n\ndocker run -d ubuntu\ndocker container ls -a\n\ndocker run -d -P --name nginx1 nginx:alpine\n\ndocker container stop nginx1\n\ndocker run --rm debian echo \"hello world\"\n\nNginx, pronounced like ‚Äúengine-ex‚Äù, is an open-source web server that, since its initial success as a web server, is now also used as a reverse proxy, HTTP cache, and load balancer."
  },
  {
    "objectID": "slides/workshop_full_slides.html#container-debugging",
    "href": "slides/workshop_full_slides.html#container-debugging",
    "title": "DevOps for Data Scientists",
    "section": "Container Debugging",
    "text": "Container Debugging\n\ndocker run -it -d ubuntu\ndocker container ls -a \ndocker exec -it CONTAINER_ID bash\n\ndocker container run -d --name mydb \\\n --name mydb \\\n -e MYSQL_ROOT_PASSWORD=my-secret-pw \\ \n mysql\n \n docker container logs mydb\n\nüîç Live code"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---debugging-containers",
    "href": "slides/workshop_full_slides.html#lab-activity---debugging-containers",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - Debugging Containers",
    "text": "Lab Activity - Debugging Containers\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 2: Part 2"
  },
  {
    "objectID": "slides/workshop_full_slides.html#port-mapping-with-docker-run--p-hostcontainer",
    "href": "slides/workshop_full_slides.html#port-mapping-with-docker-run--p-hostcontainer",
    "title": "DevOps for Data Scientists",
    "section": "Port Mapping with docker run -p host:container",
    "text": "Port Mapping with docker run -p host:container\n\n\n\ndocker pull httpd:alpine\ndocker pull httpd:latest\n\ndocker inspect --format='{{.Config.ExposedPorts}}' httpd:latest\ndocker inspect --format='{{.Config.ExposedPorts}}' httpd:alpine\n\ndocker run -p DockerHostPort:ApplicationPort\n\ndocker run -d -p 80:80 --name httpd-latest httpd:latest\ncurl http://localhost:81\n\ndocker run -d -p 6574:80 --name httpd-alpine httpd:alpine\ncurl http://localhost:80\n\nüîç Live code"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---mapping-ports",
    "href": "slides/workshop_full_slides.html#lab-activity---mapping-ports",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - Mapping ports",
    "text": "Lab Activity - Mapping ports\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 2: Part 3"
  },
  {
    "objectID": "slides/workshop_full_slides.html#persisting-data-with-docker",
    "href": "slides/workshop_full_slides.html#persisting-data-with-docker",
    "title": "DevOps for Data Scientists",
    "section": "Persisting data with Docker",
    "text": "Persisting data with Docker\n\n\nState is persistent information that is recorded and recalled later\nContainers are designed to be ephemeral e.g.¬†stateless by default\nData applications are usually stateful and more complex to deploy\n\n\nImages are made of a set of read-only layers. When we start a new container, Docker adds a read-write layer on the top of the image layers allowing the container to run as though on a standard Linux file system.\nSo, any file change inside the container creates a working copy in the read-write layer. However, when the container is stopped or deleted, that read-write layer is lost."
  },
  {
    "objectID": "slides/workshop_full_slides.html#options",
    "href": "slides/workshop_full_slides.html#options",
    "title": "DevOps for Data Scientists",
    "section": "Options",
    "text": "Options\n\n\n\n\n\n\n\n\nStorage Type\nDescription\nUse case\n\n\n\n\nVolume mount\ncreate new volume managed by Docker CLI\ndedicated filesystem on the host (/var/lib/ docker/volumes)\nNeed to transfer, share, data between multiple containers\n\n\nBind mount\nuse any available host filesystem\nnot managed by Docker CLI\nAccess hidden files\n\n\ntmpfs\nuses host memory\nnot permanent\nsensitive data\n\n\nExternal Database\ncontainer remains stateless;\ncontainer connects to external database\nProduction workflow\n\n\nExternal Shared File System\nshared network file system int the cloud (NFS)\nManaged or multi-cluster deployments\nDon‚Äôt want to limit storage"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---persisting-data",
    "href": "slides/workshop_full_slides.html#lab-activity---persisting-data",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - Persisting Data",
    "text": "Lab Activity - Persisting Data\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 2: Part 4"
  },
  {
    "objectID": "slides/workshop_full_slides.html#building-docker-images",
    "href": "slides/workshop_full_slides.html#building-docker-images",
    "title": "DevOps for Data Scientists",
    "section": "Building Docker Images",
    "text": "Building Docker Images\n\nImages are build using a Dockerfile or interactively ‚Äúon-the-fly‚Äù for testing"
  },
  {
    "objectID": "slides/workshop_full_slides.html#build-command",
    "href": "slides/workshop_full_slides.html#build-command",
    "title": "DevOps for Data Scientists",
    "section": "Build Command",
    "text": "Build Command"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity---commit-and-push-container-to-dockerhub",
    "href": "slides/workshop_full_slides.html#lab-activity---commit-and-push-container-to-dockerhub",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity - Commit and push container to DockerHub",
    "text": "Lab Activity - Commit and push container to DockerHub\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 2: Part 5"
  },
  {
    "objectID": "slides/workshop_full_slides.html#a-few-dockerfile-build-commands",
    "href": "slides/workshop_full_slides.html#a-few-dockerfile-build-commands",
    "title": "DevOps for Data Scientists",
    "section": "A few Dockerfile Build Commands",
    "text": "A few Dockerfile Build Commands\n\n\n\n\n\n\n\nCommand\nDescription\n\n\n\n\nARG\nDefine variables passed at build time\n\n\nFROM\nBase image\n\n\nENV\nDefine variable\n\n\nCOPY\nAdd local file or directory\n\n\nRUN\nExecute commands during build process\n\n\nCMD\nExecute command when you run container; once per Dockerfile\n\n\nENTRYPOINT\nExecute command to change default entrypoint at runtime\n\n\nUSER\nSet username or ID\n\n\nVOLUME\nMount host machine to container\n\n\nEXPOSE\nSpecify port pn which container listens at runtime"
  },
  {
    "objectID": "slides/workshop_full_slides.html#dockerfile-example",
    "href": "slides/workshop_full_slides.html#dockerfile-example",
    "title": "DevOps for Data Scientists",
    "section": "Dockerfile Example",
    "text": "Dockerfile Example\nWalk through Posit Connect Dockerfile"
  },
  {
    "objectID": "slides/workshop_full_slides.html#activity---putting-it-all-together-in-dockerfile",
    "href": "slides/workshop_full_slides.html#activity---putting-it-all-together-in-dockerfile",
    "title": "DevOps for Data Scientists",
    "section": "Activity - Putting it all together in Dockerfile",
    "text": "Activity - Putting it all together in Dockerfile\nüü• - I need help\nüü® - I‚Äôm still working\nüü© - I‚Äôm done\nComplete Lab 2: Part 5"
  },
  {
    "objectID": "slides/workshop_full_slides.html#part-4-data-science-in-production",
    "href": "slides/workshop_full_slides.html#part-4-data-science-in-production",
    "title": "DevOps for Data Scientists",
    "section": "Part 4: Data Science in Production",
    "text": "Part 4: Data Science in Production"
  },
  {
    "objectID": "slides/workshop_full_slides.html#data-science-in-production",
    "href": "slides/workshop_full_slides.html#data-science-in-production",
    "title": "DevOps for Data Scientists",
    "section": "Data Science in Production",
    "text": "Data Science in Production"
  },
  {
    "objectID": "slides/workshop_full_slides.html#production-state",
    "href": "slides/workshop_full_slides.html#production-state",
    "title": "DevOps for Data Scientists",
    "section": "Production ‚ÄúState‚Äù",
    "text": "Production ‚ÄúState‚Äù\n\nWhat are some questions that we should ask once our code is ready for production?\n\n\nWhere is it deployed?\nIs it secure and accessible?\nDoes it scale?\nIs it tested?"
  },
  {
    "objectID": "slides/workshop_full_slides.html#choosing-the-right-presentation-layer",
    "href": "slides/workshop_full_slides.html#choosing-the-right-presentation-layer",
    "title": "DevOps for Data Scientists",
    "section": "Choosing the right presentation layer",
    "text": "Choosing the right presentation layer\n\nAlex‚Äôs flow chart"
  },
  {
    "objectID": "slides/workshop_full_slides.html#choosing-a-rest-api",
    "href": "slides/workshop_full_slides.html#choosing-a-rest-api",
    "title": "DevOps for Data Scientists",
    "section": "Choosing a REST API",
    "text": "Choosing a REST API\n\n\napplication programming interface clients to communicate with a server. Rest API is a kind of web-service which stores and retrieves necessary data. It provides great flexibility to developers since it does not need any dependent code libraries to access the web-services. Amongst the many protocols supported by REST, the most common one is HTTP. When a request is sent from the client using a HTTPRequest, a corresponding response is sent from the server using HTTPResponse."
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity-host-api-on-posit-connect",
    "href": "slides/workshop_full_slides.html#lab-activity-host-api-on-posit-connect",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity: Host API on Posit Connect",
    "text": "Lab Activity: Host API on Posit Connect\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 3: Part 1"
  },
  {
    "objectID": "slides/workshop_full_slides.html#how-is-it-secured",
    "href": "slides/workshop_full_slides.html#how-is-it-secured",
    "title": "DevOps for Data Scientists",
    "section": "How is it secured?",
    "text": "How is it secured?"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity-explore-api-structure",
    "href": "slides/workshop_full_slides.html#lab-activity-explore-api-structure",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity: Explore API structure",
    "text": "Lab Activity: Explore API structure\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 3: Part 2"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity-explore-plumber",
    "href": "slides/workshop_full_slides.html#lab-activity-explore-plumber",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity: Explore Plumber",
    "text": "Lab Activity: Explore Plumber\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 3: Part 3"
  },
  {
    "objectID": "slides/workshop_full_slides.html#where-is-it-deployed",
    "href": "slides/workshop_full_slides.html#where-is-it-deployed",
    "title": "DevOps for Data Scientists",
    "section": "Where is it deployed?",
    "text": "Where is it deployed?"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity-push-button-deployment",
    "href": "slides/workshop_full_slides.html#lab-activity-push-button-deployment",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity: Push button deployment",
    "text": "Lab Activity: Push button deployment\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 3: Part 4"
  },
  {
    "objectID": "slides/workshop_full_slides.html#lab-activity-access-api-data",
    "href": "slides/workshop_full_slides.html#lab-activity-access-api-data",
    "title": "DevOps for Data Scientists",
    "section": "Lab Activity: Access API data",
    "text": "Lab Activity: Access API data\n\nüü• - I need help\nüü© - I‚Äôm done\n\nLab 3: Part 5"
  },
  {
    "objectID": "slides/workshop_full_slides.html#thank-you-so-much",
    "href": "slides/workshop_full_slides.html#thank-you-so-much",
    "title": "DevOps for Data Scientists",
    "section": "Thank you so much!!!!",
    "text": "Thank you so much!!!!\n\n\n\n\nhttps://github.com/posit-conf-2023/DevOps"
  }
]